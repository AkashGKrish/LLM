{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'OAK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents=SimpleDirectoryReader(\"Data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='a708a59b-b19f-4ca7-9e82-1e62eaa5ba01', embedding=None, metadata={'page_label': '', 'file_name': '2307.01658v1.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/2307.01658v1.pdf', 'file_type': 'application/pdf', 'file_size': 666755, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='SliceOps: Explainable MLOps for Streamlined\\nAutomation-Native 6G Networks\\nFarhad Rezazadeh, Student Member, IEEE , Hatim Chergui, Senior Member, IEEE ,\\nLuis Alonso, Senior Member, IEEE , and Christos Verikoukis, Senior Member, IEEE\\nAbstract —Sixth-generation (6G) network slicing is the backbone\\nof future communications systems. It inaugurates the era of\\nextreme ultra-reliable and low-latency communication (xURLLC)\\nand pervades the digitalization of the various vertical immersive\\nuse cases. Since 6G inherently underpins artificial intelligence (AI),\\nwe propose a systematic and standalone slice termed SliceOps that\\nis natively embedded in the 6G architecture, which gathers and\\nmanages the whole AI lifecycle through monitoring, re-training,\\nand deploying the machine learning (ML) models as a service for\\nthe 6G slices. By leveraging machine learning operations (MLOps)\\nin conjunction with eXplainable AI (XAI), SliceOps strives to cope\\nwith the opaqueness of black-box AI using explanation-guided\\nreinforcement learning (XRL) to fulfill transparency, trustworthi-\\nness, and interpretability in the network slicing ecosystem. This\\narticle starts by elaborating on the architectural and algorithmic\\naspects of SliceOps. Then, the deployed cloud-native SliceOps\\nworking is exemplified via a latency-aware resource allocation\\nproblem. The deep RL (DRL)-based SliceOps agents within slices\\nprovide AI services aiming to allocate optimal radio resources and\\nimpede service quality degradation. Simulation results demon-\\nstrate the effectiveness of SliceOps-driven slicing. The article dis-\\ncusses afterward the SliceOps challenges and limitations. Finally,\\nthe key open research directions corresponding to the proposed\\napproach are identified.\\nIndex Terms —6G, AI, MLOps, network slicing, resource allo-\\ncation, XAI, XRL, zero-touch\\nI. I NTRODUCTION\\n6G slicing is envisioned as a disruptive technology that\\nintelligently supports various verticals with different quality\\nof service (QoS) requirements. Such massive slicing is viewed\\nas a new paradigm in 6G network that supports numerous slices\\nwith micro or macro services. Consequently, the complexity of\\nautomated management and orchestration (MANO) operations\\nwould arise dramatically. The tendency towards fully automated\\nMANO in beyond 5G/6G has spurred intensive research interest\\nin applying AI and ML as an ideal solution for various nonlin-\\near problems. Notably, novel practices are required to deploy\\nML solutions into production and providing a trustworthy and\\nF. Rezazadeh is with the Telecommunications Technological Center of Cat-\\nalonia (CTTC) and Technical University of Catalonia (UPC), 08860 Castellde-\\nfels, Spain (e-mail: frezazadeh@cttc.es).\\nH. Chergui is with the i2CAT Foundation, 08034 Barcelona, Spain (e-mail:\\nchergui@ieee.org).\\nLuis Alonso is with the Technical University of Catalonia (UPC), 08860\\nCastelldefels, Spain (e-mail: luisg@tsc.upc.edu).\\nC. Verikoukis is with the University of Patras, ATHENA/ISI,\\nGreece, and IQUADRAT Informatica, Barcelona 08006, Spain (e-mail:\\nchverik@gmail.com).actionable AI-driven slicing environment. In this intent, the\\nML model deployed in production should inescapably undergo\\nthe combination of development and operations (DevOps) and\\nML Operations (MLOps). On the other hand, following the\\ntechnical report of the European Commission on \"Ethics guide-\\nlines for trustworthy AI\" [1], the AI solutions should pursue\\ntrustworthiness. Indeed, the XAI approach scrutinizes each\\nfeature and its impact on the output of the AI model, enabling\\nto observe the factors that either positively or negatively impact\\nthe AI model prediction.\\nIn this regard, XRL is viewed as a responsible and trustful\\nML approach that can be combined with MLOps lifecycle.\\nIndeed, in the reinforcement learning (RL) method, the agent\\ngenerates the corresponding dataset on the fly by interacting\\nwith the network. This method is an evaluative and feedback-\\nbased learning to optimize the accumulated long-term reward.\\nDespite the promising results and performance, there is a\\nconcern about the essence of deep neural network (DNN) that\\nare deemed as opaque models. This issue is exacerbated when\\nhigh reliability and security play a vital role in realistic network\\nscenario and impedes users from trusting the trained agents and\\npredicted results in the network slicing ecosystem. Moreover,\\nthe conflict-prone nature of state-action can be highly crippling\\nthe promising RL solutions in automated network slicing.\\nMotivated by explanation-guided learning (EGL), we consider\\nan intrinsic interpretability approach in the training phase of the\\nproposed SliceOps agent. In this intent, XAI can assist DRL to\\nextract more relative state-action pairs where it explains which\\nstate or input of agent has the most positive impact on action\\nor decision.\\nTo incorporate these principles into a single design while en-\\nsuring a separation of concerns, this paper introduces SliceOps ,\\nan XAI-empowered MLOps framework that is natively embed-\\nded in the 6G network architecture as a standalone slice. To\\nillustrate its operation, we consider a latency-aware resource al-\\nlocation problem where each slice registers to the corresponding\\nSliceOps instance which provides AI services via a SliceOps\\nagent. The main goal is to allocate optimal radio resources to\\nthe slices while minimizing the latency to meet the service level\\nagreements (SLAs). The following contributions are presented\\nin this paper:\\n•We introduce the architecture of SliceOps, where the\\nexplainable ML operations are gathered in a standalone\\nslice providing AI services to the rest of the slices. ThisarXiv:2307.01658v1  [cs.NI]  4 Jul 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62b457f3-f467-4ece-ac33-e48e4f9eee9c', embedding=None, metadata={'page_label': '', 'file_name': '2307.01658v1.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/2307.01658v1.pdf', 'file_type': 'application/pdf', 'file_size': 666755, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='continuous delivery (CD) and continuous integration (CI)\\nof ML models enhances reliability and interpretability\\nwhile quickly deploying AI models in the network with\\nhigher consistency.\\n•As a use case of SliceOps, a RAN resource allocation\\nproblem is defined, aiming at reducing SLA violations.\\n•To solve this problem, SliceOps agents are proposed,\\nwhich are based on a novel explanation-guided DRL\\n(XRL) scheme, that is assisted with shapley additive\\nexplanations (SHAP) importance values and an entropy\\nmapper to guide the agent in reducing uncertainty in its\\nactions across various network states.\\n•The AI and network analysis demonstrate the superiority\\nand faithfulness of the proposed explanation-guided DRL\\napproach compared to the RL baseline.\\nFor the sake of exploring the aspects mentioned above, the ar-\\nticle starts by highlighting the background of this work. Follow-\\ning that, we discuss the explainability in RL and the interactions\\nbetween the different components of the SliceOps workflow.\\nWe illustrate the performance analysis and effectiveness of the\\nproposed SliceOps-driven resource allocation. Also, we reveal\\nthe challenges and limitations of the proposed AI-native slice.\\nFinally, the open research directions for implementation of 6G\\nSliceOps are highlighted and concludes the article.\\nII. R ESEARCH AND STANDARDIZATION WORK RELEVANT\\nTOMLO PS\\nThe authors in [2] developed an edge MLOps framework\\nfor automating and more efficient artificial intelligence of\\nthings (AIoT) operations and decision-making. The proposed\\nframework aims to operationalize the CD and CI of ML models\\nto the nodes as an essential part of DevOps. Samaras et al.\\n[3] proposed a cloud-native MLOps automation platform for\\nautomated and optimized network slice lifecycle management\\n(LCM). This event-based framework is a zero-touch MLOps\\nplatform that automatically analyzes its accuracy performance\\nand adapts itself automatically through the training job. In [4],\\nthe authors presented an ML-as-a-Service (MLaaS) approach\\nfor 5G IoT based on a TinyMLaaS (TMLaaS) architecture.\\nThey leveraged an MLOps framework for unifying ML systems\\nto implement, deploy, and maintain operations. The authors\\nin [5] described two levels of MLOps in open radio access\\nnetwork (O-RAN). They considered a deep learning (DL)-\\nbased MLOps and leveraged the DevOps principles to ease\\nML system development (Dev) and ML system operations\\n(Ops) for O-RAN automation. Correspondingly, [6] proposed\\nan MLOps lifecycle based on RL aiming to automate and\\nreproducible model development process in the O-RAN deploy-\\nment. The proposed scheme introduces principles and practices\\nfor developing data-derived optimal decision-making strategies\\nTsourdinis et al. [7] developed a service-aware dynamic slice\\nallocation scheme. The proposed AI/ML unit in the pipeline\\nfollows an MLOps-based distributed ML architecture. Recently,\\nreward shaping [8] and attention mechanisms [9] have emerged\\nas prominent techniques in enhancing the explainability ofDRL methods. Reward shaping entails altering the rewards\\nthat an agent receives to offer supplementary guidance to the\\nagent. Moreover, attention mechanisms empower the agent\\nto concentrate on the pertinent network states and inputs. It\\npresents the importance of each feature or state in state-action\\npairs for effective decision-making.\\nAlongside research contributions, some organizations’ ini-\\ntiatives are relevant for MLOps components in telecommuni-\\ncations. The European telecommunications standards institute\\nexperiential networked intelligence (ETSI ENI) is defining\\na cognitive network management architecture leveraging AI\\ntechniques. The architecture aims to provide fully-automated\\nservice provision, operation, and assurance. The corresponding\\nclosed-loop AI mechanisms can be leveraged in the MLOps\\nlifecycle. Unlike ETSI ENI, which focuses on AI techniques,\\nEuropean telecommunication standards institute zero-touch ser-\\nvice management (ETSI ZSM) [10] is investigating automation\\nchallenges faced by operators and vertical industries. The\\ngroup works on end-to-end (E2E) architecture and services\\nautomation to solve radical changes in the way networks are\\nmanaged and orchestrated in the pivotal deployment of 5G\\nand network slicing. In this intent, an MLOps approach with\\nlifecycle management of ML models for reproducible ML\\npipelines would be necessary.\\nThere are still several issues to be solved. The stakeholders\\nin structuring network slices need innovations and adjustments\\nto embed MLOps into the network while enhancing the in-\\nterpretability of the black-box AI decision-making process to\\nattain human trust. To this end, we propose a revolutionary\\napproach for 6G networks, called SliceOps with an attention-\\nbased submodule to inspect the contribution and impact of\\nnetwork slice states on decision-making and choice of particular\\nactions.\\nIII. P ROPOSED EXPLAINABLE MLO PSSLICING\\nA. SliceOps Lifecycle\\nThe SliceOps foundation relies on a practice aiming to stan-\\ndardize production methods through incorporating the concept\\nof CI and CD, producing thereby reliable software and AI\\nsolutions in short cycles. In 6G AI-native networks, SliceOps\\nmodular design allows the evolution, upgrade, and scaling of\\nthe MLOps layer and its AI functions separately from the\\nservice layers. Fig. 1 depicts the pictorial representation of\\nan operationalizing AI-native slice deployed at the top of the\\nnetwork slicing environment with multiple SliceOps instances.\\nThe instances provide AI services to the corresponding slice\\n(e.g., URLLC), and they can collaborate for specific tasks\\nlike resource allocation, where resources are generally shared\\nbetween service slices. On the other hand, SliceOps guarantees\\nAI performance isolation through triggering re-training, which\\nmeans that if a sliceOps instance degrades for one slice, it will\\nhave minimum effect on the other slices. This is in line with\\ncontainerized solutions that decouple the execution of iterative\\nprocesses which is a necessity in ML pipelines. The SliceOps', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f815b035-7f9d-4046-a78a-39379b5654a5', embedding=None, metadata={'page_label': '', 'file_name': '2307.01658v1.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/2307.01658v1.pdf', 'file_type': 'application/pdf', 'file_size': 666755, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Model LoadingModel Training Environment Twin\\nModel ExplanationInterpretability Metric\\nEvaluationModel SavingNo\\nModel\\nRegistry\\nTrained Model\\nServingPrediction Service\\n(XAI-as-Service)Data Engineering and\\nEnv. Twin Update\\nComposite\\nReward suf ficient?eMBBmMTC\\nURLLCMonitoring\\nInformation34\\n6\\n72\\n1\\nKafka ClusterManager\\nMonitoring SystemNo\\nURLLC\\neMBB\\nmMTC\\nLive Network Slicing Environment8\\nDeploy\\nModelsComposite\\nReward suf ficient?YesAI-Native Slice (SliceOps)\\n5\\nYesFigure 1: The SliceOps workflow comprises of monitoring, re-training, and deploying ML model as a service.\\nadheres to the below fundamental pipeline principles based on\\nRL algorithms:\\n1) Monitoring System and Data Collection (1): It is con-\\nsidered as a backbone step in ML practices where data ac-\\nquisition is utilized for data preparation and model design\\nprocesses. It allows SliceOps agents that run as containers\\ninstances within slices to collect real-time monitoring data\\nfrom the gNodeB (gNB) platform of multiple key performance\\nindicators (KPIs), encompassing available resources, number\\nof connected devices, bandwidth utilization, channel quality,\\netc. This component can store structured and unstructured data\\non a very large scale. Such data is streamed through e.g., a\\nKafka bus to which the SliceOps AI functions can subscribe\\nand fetch the relevant data under a specific Kafka topic name.\\nThe obtained data is used as input of the pipeline to guide\\nthe definition of policy in the form of service prediction. The\\nSliceOps RL-based agent collects data on-the-fly (online RL)\\ninteracting with the slice environment or initializes the process\\nwith a pre-collected dataset (offline RL) stored in an internal\\ndatabase. The online RL can bring additional risks in terms of\\ninteracting with live environment and collected data. To solve\\nthis, we consider step 2, 6, and 8.\\n2) Data Engineering and Model Loading (2, 3): This com-\\nponent is responsible for data preprocessing or preparation.\\nDifferent optimization targets and AI services require hetero-\\ngeneous training data for neural networks. This process takes\\nraw data from the monitoring system and transforms it into an\\nunderstandable format for RL such as OpenAI Gym. The raw\\ndata contains errors and inconsistencies while having various\\nattributes or patterns. For example, the monitored data ofdifferent slice domains can be non-Euclidean or not meeting\\nindependent and identically distributed (IID) dataset features.\\nWith the various forms of data in network slicing, pursuing the\\ntechniques such as assessment of data quality, data cleaning,\\ndata transformation, reduction of data, etc., is a vital step\\nfor better learning performance. The next step is to create\\nneural network architecture and hyperparameters tuning before\\ncompiling and loading them into a model. The hyperparameter\\ntuning depends highly on capability, scenario, and technology\\nused [11]. Different datasets require setting different hyper-\\nparameters to guide the model to predict accurately in the\\nfollowing steps.\\n3) Model Training, Evaluation and Saving (4, 5, 6): This\\nmodule of SliceOps is segregated into a set of processes\\nfor the execution of continuous model training automatically\\nwith processed data. The ML model training runs a local\\noptimization task, while the model explanation involves an\\nexplainer , either attribution-based (e.g., Integrated Gradient,\\nSaliency Maps) or perturbation-based (e.g., SHAP [12]). Upon\\nthe evaluation of RL reward including interpretability metric,\\nthe model retraining is triggered whenever there is a model per-\\nformance deterioration following new training data arrival (new\\ninexplored states). Note that the reward should also correlate\\nwith slice targets (SLA, KPIs, etc.), while the interpretability\\nrefers to XAI metrics (attributions-based entropy, confidence,\\nlog-odds, fidelity, etc.) depending on the SLA adopted by\\nthe slice tenant. In this respect, a feedback loop between\\nthe explainer and the model is necessary to feed the model\\noptimizer with the measured XAI metrics, enabling thereby', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dcafa296-18a5-4785-9d74-55a3d00c31cc', embedding=None, metadata={'page_label': '', 'file_name': '2307.01658v1.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/2307.01658v1.pdf', 'file_type': 'application/pdf', 'file_size': 666755, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 2: The workflow of SliceOps model training based on explain-\\nable DRL.\\nexplainability-aware learning. Finally, in case the model fulfills\\nthe target performance, it will be registered and stored in the\\nmodel registry to be promoted into production later on.\\n4) AI Service Provisioning (7, 8): The next step after\\ntraining and evaluation is to retrieve the registered model for\\nencapsulation and move to the production stage in the form of\\na model prediction service. The trained model is continuously\\nexposed and delivered to slices as representational state transfer\\n(REST) application programming interface (API) assisted with\\nSliceOps agents. The framework considers continuous monitor-\\ning in different steps to ensure model performance.\\nB. Explanation-Guided RL Building Blocks\\nThe aforementioned agents are assumed to be DRL-based.\\nUnlike the conventional DRL where there is no causal relation\\nbetween the input state parameters and the output action,\\nthis paper introduces an attention mechanism based on SHAP\\nexplainer, which quantifies the relevance of a state to the action\\nand guides the XRL agent to perform explainable decisions\\nthrough XAI-augmented reward shaping. The training workflow\\nof the proposed XRL scheme is illustrated in Fig 2, which is\\ncomposed of the following main components,\\n1) Explainer: It explains the DRL decision by assigning\\nhigh scores (in absolute value) to the most influencing input\\nstate parameters. The score corresponds to the SHAP value,\\nwhich is computed using e.g., a perturbation-based approach.\\nSpecifically, each feature is perturbed or modified while keep-\\ning other features fixed at their baseline values (e.g., white\\nnoise, zero). The model’s response is observed by evaluat-\\ning the perturbed instances and recording the corresponding\\npredictions. The differences between the predictions of the\\nperturbed instances and the baseline prediction are computed\\nto capture the contribution of each feature when changed\\nfrom the baseline value. In Sec. IV-B, we demonstrate these\\ncontributions are aggregated across different perturbations to\\nestimate the SHAP values. Following the DRL agent interaction\\nwith the Environment Twin, it temporarily saves the experiences\\nand observations in a replay memory/buffer which is steadily\\nupdated. Then, it generates the SHAP importance values over\\nan extracted batch dataset of state-action.\\n2) Entropy Mapper: It applies a softmax layer to the SHAP\\nvalues provided by the Explainer and consequently generates\\na probability distribution, which is used afterward to calculatethe entropy that measures uncertainty of the taken action given\\nthe input state.\\n3) Composite Reward Signal: The obtained multiplicative\\ninverse of the maximum entropy value is used as XAI reward.\\nIn Sec. IV-B, we showcase that the composite reward—which\\nis a sum of the SLA reward (based on meeting or violating the\\nSLA requirements) and the XAI reward—results in minimizing\\nthe uncertainty of state-action pairs and encouraging the agent\\nto select the best actions for specific network state values. This\\napproach can elucidate the learning process while directing\\nthe learning toward making explainable decisions concerning\\na specific state.\\nC. Benefits of SliceOps\\nThe main objectives of segregating the control plane from\\nthe user plane in 5G and beyond 5G networks encompass\\nscalability, flexibility, and agility to streamline the development\\nof new services and use cases. The proposed SliceOps approach\\naligns with this philosophy by creating an innovative standalone\\nAI plane to independently provide AI services and functions to\\nthe rest of the network slices. It eliminates the necessity to\\nmodify the underlying AI functions and structure of network\\nslice instances for new services. By a decoupled AI plane from\\nthe control plane and user plane, the network can upgrade or\\nadd new AI and automation functions without impacting other\\nplanes. This flexibility and mentioned explainability features of\\nSliceOps pave the way for faster deployment and trustworthy\\nnetwork optimizations in network slicing.\\nD. ETSI ZSM Compliance\\nThe design of SliceOps solution closely adheres to the ZSM\\nframework. The standardization process for ETSI ZSM is still\\nin its early stages, with preliminary specifications based on\\na high level of abstraction. The core concept of the closed-\\nloop AI system is to utilize context-aware and metadata-driven\\npolicies to more efficiently and quickly identify and incorporate\\nnew conditions while updating knowledge and making robust\\nand actionable decisions. As shown in Fig. 1, SliceOps is a\\npractice toward deploying a more realistic closed-loop scheme\\nto fulfill viable automation solutions for network slicing control.\\nSliceOps manages the lifecycle of AI models in a closed-loop\\nway that includes model performance monitoring, re-training,\\nand delivery. It extends the ZSM framework to manage, besides\\nthe service functions, the underlying AI functions, which are\\nnatively supported in a standalone slice. Moreover, the SliceOps\\nagents are based on the closed-loop workflow in Fig. 2.\\nE. O-RAN Slicing Compliance\\nThe SliceOps framework can be adapted to use case 3 of\\nopen radio access network (O-RAN) slicing, which is related to\\nresource allocation optimization, requirements, and architecture\\n[13]. Specifically, SliceOps layer can be developed as rApps at\\nthe non-real-time (Non-RT) radio intelligent controller (RIC).\\nThanks to its architecture that brings valuable differentiators by', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='74b8df15-6456-412f-b399-98df730805c2', embedding=None, metadata={'page_label': '', 'file_name': '2307.01658v1.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/2307.01658v1.pdf', 'file_type': 'application/pdf', 'file_size': 666755, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='leveraging XAI and CI/CD, SliceOps would implement new\\nuse cases with agility and automate network operations. In\\nthis respect, rApps need to access abundant monitoring data—\\nsuch as traffic load, latency, and signal strength—through the\\nO1 interface to efficiently carry out their designated functions\\nin time-demanding training procedures. Then, the model or\\npolicy trained by SliceOps can be packaged as an artifact and\\ndelivered via the A1 interface to run on the Near-RT RIC\\ninterface as xApp. This model deployment is performed using\\nREST APIs over HTTP for the transfer of such JSON objects.\\nFor dynamic network optimization purposes, this xApp would\\ncontrol the underlying O-RAN components, namely, the central\\nunit-control plane (O-CU-CP), different slice open central unit-\\nuser plane (O-CU-UP), and open distributed unit plane (O-DU)\\nby using the E2 interface. The E2 interface encompasses two\\nprotocols, namely the E2 application protocol (E2AP) and the\\nE2 service model (E2SM).\\nIV. E VALUATION\\nThe next important step after introducing the key elements of\\nthe proposed framework is to exemplify the SliceOps approach\\nand evaluate the performance to verify the benefits of XAI in\\nthe MLOps pipeline. In this section, we demonstrate the im-\\nplementation of a small-scale SliceOps framework for network\\nslicing and then the effectiveness of the model is validated in\\nterms of ameliorating the long-term revenue (average reward),\\ntransmission latency, and dropped traffic.\\nA. Network Architecture and Experiment Parameters\\nFor the sake of validating the SliceOps framework in realistic\\nsettings, we consider a gNB scenario, wherein a set of slices I\\nis deployed. The scenario includes three slices, i.e., URLLC,\\nenhanced mobile broadband (eMBB), and massive machine-\\ntype communications (mMTC). The slices are characterized by\\nthe SLA latency Λi= [10 ,40,20]ms, respectively. Without\\nloss of generality, the considered gNB is characterized by the\\nradio capacity C= 100 physical resource blocks (PRBs) of\\na fixed bandwidth and assumes the slices running over gNB\\nsimultaneously. The slice traffic demand is modeled as the\\nrealization of a Poisson distribution with mean value λiand\\nemulates the signal-to-noise ratio (SNR) variability extracting\\nits instantaneous values from a Rayleigh distribution with the\\naverage value set to 25dB. We set ι= 10 PRBs as the\\nminimum resource allocation step. The framework leverages\\nPython programming language, exploiting OpenAI Gym li-\\nbrary [11] and interfacing DRL agents with a custom gNB\\nsimulator environment [14]. The simulator consists of virtual\\ntransmission queues and main physical (PHY), medium access\\ncontrol (MAC), and radio link control (RLC) functionalities.\\nEach SlicOps agent is endowed with a double deep Q-network\\n(DDQN) [15]. The agents interact with each other and network\\nenvironment to gather the slice networking statistics (e.g.,\\nchannel quality, served traffic, consumed resources, etc.). Then,\\nthey enforce PRB policy decisions provided by the corre-\\nsponding SliceOps layer in the gNB slice scheduler. We usea dedicated server equipped with two Intel(R) Xeon(R) Gold\\n5218 CPUs @ 2.30GHz, two NVIDIA GeForce RTX 2080 Ti\\nGPUs, and the DNNs are implemented based on TensorFlow-\\nGPU version 2.5.0. The neural network architecture uses two\\nfully connected layers with 24neurons activated by ReLU\\nfunction. The network parameters are updated using the Adam\\noptimizer. The discount factor γand learning rate ξare set\\nto be 0.99 and 0.001 , respectively. The replay buffer size of\\neach agent βiis20000 samples, out of which a batch of 32\\nsamples is extracted for each training interval. To deploy the\\nsolution in the cloud-native mode, we leveraged a containerized\\napproach where a cloud server hosts SliceOps instances and\\ncorresponding modules responsible for providing AI service\\nfor different problems in slices. On the other hand, SliceOps\\nagents of slices run by using the Docker compose tool and\\ncommunicate with the server through FastAPI as a REST API.\\nThe operational training phases are discussed in Sec. III-B.\\nB. Latency-Aware Resource Allocation\\nAcquiring swift and constructive resource allocation in net-\\nwork slicing is precluded due to the lack of dynamic traffic\\nsteering. We cast the radio resource allocation problem in\\ngNB as an optimization problem, emphasizing on minimizing\\nallocated resources and latency to meet the SLA. We consider\\nthe transmission latency as the average time that traffic of a slice\\nexperiences before being served within the gNB transmission\\nbuffers due to the inter-slice scheduling process. The radio\\nresource availability for the downlink traffic is divided into\\nsubsets of PRBs. The provided AI model by SliceOps instances\\nlets the SliceOps agent dynamically assign the optimum PRBs\\nto each network slice following the real-time traffic and SLA\\nrequirements. In this scenario, we consider correct and fair\\ndimensioning of the inter-slice PRB enforcement instead of\\nfocusing on the intra-slice scheduling issue.\\nFigure 3: Convergence performance of the RL and XRL approaches.\\nFor the sake of visual clarity, the curves are smoothed concerning\\nconfidence bands and standard deviation.\\nAs shown in Fig. 3, the designed composite reward function\\n(XRL strategy) assisted by the SHAP approach guarantees\\nbetter learning generalization and robust performance compared\\nto the RL method (SLA reward) as the baseline. In around\\nhalf of the overall training, the eMBB SliceOps agent inspects\\naction space (PRBs) that initially leads to high fluctuations in\\nlearning curves, i.e., exploration, and then strives to achieve', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='414599d3-310f-404d-a72a-a6c3584ed089', embedding=None, metadata={'page_label': '', 'file_name': '2307.01658v1.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/2307.01658v1.pdf', 'file_type': 'application/pdf', 'file_size': 666755, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='the right trade-off between the learned decision policies and\\nvarying network states, i.e., exploitation. It is safe to assert\\nthat the composite reward based on the proposed explanation-\\nguided action-selection strategy enhances the performance of\\nSliceOps agents. The SHAP explainer extracts the features and\\ntheir importance values from the batch dataset for a particular\\nprediction in conjunction with the entropy mapper to provide a\\nreward metric to guide the agent with more relevant state-action\\npairs.\\nFigure 4: Network performance comparison for RL and XRL settings.\\n(Up) The transmission latency CDF for URLLC slice, (Down) The\\nperformance evaluation in terms of dropped traffic for mMTC slice.\\nFig. 4 (Up) depicts the cumulative distribution function\\n(CDF) of the time that URLLC traffic experiences within the\\ngNB transmission buffer, resulting from RL and XRL-based\\nSliceOps resource allocation. From the results, it can be noticed\\nthat the XRL approach leads to higher performance where\\n50% of perceived latencies in the URLLC slice is less than\\n1.5 ms , whereas this value for the RL solution is 3.75 ms .\\nThe performance of XRL-SliceOps agents reveals that they\\nallocate adequate radio resources proportional to the traffic\\ndemand while handling varying resource contention among\\nslices. In contrast, the defective collaboration among agents in\\nthe RL strategy and the erroneous trade-off between resource\\nallocation and network conditions (state-action pairs) result in\\nhigher incurred latency.\\nWe continue the performance analysis on the proposed\\nXRL approach by shedding light on the volume of dropped\\ntraffic that does not meet latency SLA requirements owing to\\nmistaken radio resource allocation policies, as showcased by\\nFig. 4 (Down). The box plot illustrates the highest value of\\nmMTC dropped traffic, excluding outliers for the XRL scheme,\\nis4.8% whereas this value for the RL method is 11.7% .\\nBesides, the lopsided box plot of XRL is positively skewed\\nwhere the mean value is greater than the median, i.e., the\\nmajority of the values is located on the left side (lower dropped\\ntraffic values). In contrast, mMTC slice experiences higher\\ndropped traffic by RL solution where the box plot indicatesa few exceptionally small dropped traffic and most values are\\nlarge, which results in the mean being pulled to the left.\\nRL XRL2.55.07.510.012.515.017.520.0Time Efficiency (s)\\nFigure 5: Comparison of algorithms in terms of time efficiency on the\\nnetwork slicing setup. All evaluations were conducted on the defined\\nserver in Sec. IV-A\\nFig. 5 reflects time efficiency comparison between RL and\\nXRL algorithms during the whole training operation. The box\\nplot illustrates that the median line of the box for RL and XRL\\nis at 6sand 16s, respectively. At a glance, we can explicate\\nthat the RL algorithm takes less processing time to complete\\nepisodes than XRL. However, as shown in Fig. 3, XRL con-\\nverges faster, which can compensate this higher complexity\\ncompared to the RL approach. Note that there are some outliers\\nfor each box between 2.5s and3s, which is the period of filling\\nthe replay buffer initially as a part of training different DRL\\nalgorithms.\\n9.0 9.5 10.0 10.5 11.00.476 = Left PRBs0.177 = Served Traffic0 = SNR\\n Left PRBs Served Traffic SNR\\n+0.57\\n+0.331.77\\nExploration phase\\n(XRL-URLLC)\\nE[f(X)]=9.875f(x)=8.999\\n8 10 12 14 16 180.65 = SNR0.486 = Left PRBs0.003 = Served Traffic\\n SNR Left PRBs Served Traffic +10.34\\n+0.991.86\\nExploitation phase\\n(XRL-URLLC)\\nE[f(X)]=8.531f(x)=17.999\\nFigure 6: The XAI waterfall plot illustrates the contribution (either\\npositive or negative) of a given input state parameter to the output de-\\ncision for URLLC slice. (Up) Exploration phase, (Down) Exploitation\\nphase.\\nIn Fig. 6, we observe that f(x)represents the predicted\\naction taken by the XRL agent, which involves allocating\\nPRBs to the URLLC slice. Meanwhile, E[f(x)]represents\\nthe expected value or the average of all possible actions. The\\nabsolute SHAP value provides insights into the influence of a\\nsingle state on the action taken. During the initial stages of\\ntraining, the agent behaves as a Max C/I scheduler, resulting\\nin a penalty for URLLC users experiencing a low SNR state\\n(SHAP value =−1.77). Consequently, this leads to a relatively\\nlow allocation of PRBs per slice, specifically 8.99PRBs. Con-\\ntrastingly, in episode 500, which signifies the exploitation phase', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e939b232-e459-443b-9555-ebafd7f3edce', embedding=None, metadata={'page_label': '', 'file_name': '2307.01658v1.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/2307.01658v1.pdf', 'file_type': 'application/pdf', 'file_size': 666755, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='where the agent has learned the optimal policy, the agent’s\\naction primarily depends on the served traffic (SHAP value\\n= 10.34). As a result, a higher allocation of PRBs, specifically\\n17.99PRBs, is observed. In Fig. 6 (Up), the exploration entails\\nengaging in actions with uncertain PRBs allocation policy aims\\nto acquire deep insights about the network environment to learn\\nbetter optimal strategies. Fig. 6 (Down) presents the results in\\nexploitation phase where agent leverages the learned knowledge\\nin exploration phase and executing PRB actions that are esti-\\nmated to yield the highest rewards. When employing various\\nbackground batch sizes obtained through random sampling, the\\nSHAP values and variable rankings display fluctuations, which\\nimpedes their trustworthiness. Fortunately, as the size of the\\nbackground batch increases, these fluctuations tend to diminish.\\nV. C HALLENGES AND LIMITATIONS OF SLICE OPS\\nA. Time-Sensitive Slicing\\nMLOps can speed up the different steps of AI operations\\nin slices, such as data collection and preparation process, and\\nprovide faster response to changed conditions of the network.\\nHowever, it suffers from the extensive process and long time\\nE2E procedure. Thus, optimizing the training time and amelio-\\nrating inference model performance is necessary to harness the\\nfull potential of MLOps into a slice instance.\\nB. Secure Slicing\\nNetwork slicing may introduce new vulnerabilities, while\\nthe stringent performance of slices in a shared infrastructure\\nenvironment requires a secure E2E network. In this intent, slice\\nisolation play a vital role in guaranteeing safe and accurate\\noperations. The secure data pipelines in MLOps are challenging\\nand need to consistently set governance rules in each pipeline\\nstep to protect against different data attacks.\\nC. Collaborative Slicing\\nThe learning procedure for establishing large-scale intelligent\\nnetwork slicing is not trivial since it is time-consuming and\\ncomputationally complex, especially if each slice is indepen-\\ndently trained for the same tasks. By relying on transfer\\nlearning with a collaborative approach between slice agents, a\\npartially trained AI model can be distributed to different slices\\nwith similar tasks. Nonetheless, the sheer number of model\\ntransfer can raise privacy and isolation concerns.\\nVI. F UTURE RESEARCH DIRECTIONS\\nA. Sustainability\\nIn 6G, we will witness a massive number of slices that deal\\nwith significantly more data at faster rates than the current\\nnetwork’s deployment. It is of utmost importance to derive more\\nsuitable power-optimized solutions to ensure the long-term sus-\\ntainability of AI-driven slicing. Therefore, more research should\\nbe conducted to reduce computations and energy consumption\\nin different steps of MLOps-driven slicing.B. SliceOps-Enabling O-RAN\\nWe discussed the compliance of SliceOps in O-RAN slicing\\narchitecture [13] in Sec. III-E. Nevertheless, It represents a\\nconsiderable challenge due to the lack of principles and dis-\\ntributed architecture to underpin AI-driven massive 6G slicing.\\nIt is an exciting line of research to achieve reproducible AI\\nmodel deployment in an O-RAN massive slicing setup.\\nC. SliceOps-Enabling Metaverse Services\\nIn the ambitious vision of 6G, the coined xURLLC will\\nbecome conflated with both eMBB and mMTC. The xURLLC\\npaves the way for the implementation of immersive and Meta-\\nverse services. In this intent, the feasible MLOps solutions need\\nmore study and analysis to satisfy requirements of xURLLC\\nservices, such as latency and bandwidth.\\nVII. C ONCLUSION\\nAlgorithmic and architectural innovations are required to\\nstreamline 6G slicing automation in future networks. This\\npaper has introduced SliceOps, a framework for AI-native 6G\\nnetworks, where the AI operations (MLOps) are gathered in a\\nstandalone slice that provides AI service to the rest of slices.\\nThis AI slice extends the ZSM closed-loop to the AI lifecy-\\ncle management. Moreover, explainability-guided learning is\\nadopted in SliceOps to ensure trust in and robustness of the\\nDRL agents. Both AI and network performance results underpin\\nthe proposed framework. Finally, the challenges and future\\nresearch directions are identified.\\nREFERENCES\\n[1] High-Level Expert Group on Artificial Intelligence, “Ethics Guidelines\\nfor Trustworthy AI,,” Technical Report of the European Commission , Apr.\\n2019.\\n[2] E. Raj, D. Buffoni, M. Westerlund, and K. Ahola, “Edge MLOps: An\\nAutomation Framework for AIoT Applications,” in Proc. IEEE Int. Conf.\\non Cloud Engineering (IC2E) , Oct. 2021.\\n[3] G. Samaras, V . Theodorou, D. Laskaratos, N. Psaromanolakis, M. Mertiri,\\nand A. Valantasis, “QMP: A Cloud-native MLOps Automation Platform\\nfor Zero-Touch Service Assurance in 5G Systems,” in Proc. IEEE Int.\\nMediterranean Conf. on Communications and Networking (MeditCom) ,\\nSep. 2022.\\n[4] S. A. R. Zaidi, A. M. Hayajneh, M. Hafeez, and Q. Z. Ahmed, “Unlocking\\nEdge Intelligence Through Tiny Machine Learning (TinyML),” IEEE\\nAccess , vol. 10, pp. 100 867–100 877, Sep. 2022.\\n[5] B. Brik, K. Boutiba, and A. Ksentini, “Deep Learning for B5G Open\\nRadio Access Network: Evolution, Survey, Case Studies, and Challenges,”\\nIEEE Open Journal of the Communications Society , vol. 3, pp. 228–250,\\nJan. 2022.\\n[6] P. Li et al. , “RLOps: Development Life-Cycle of Reinforcement Learning\\nAided Open RAN,” IEEE Access , vol. 10, pp. 113 808–113 826, Oct.\\n2022.\\n[7] T. Tsourdinis, I. Chatzistefanidis, N. Makris, and T. Korakis, “AI-driven\\nService-aware Real-time Slicing for beyond 5G Networks,” in Proc. IEEE\\nINFOCOM 2022 - IEEE Conf. on Computer Communications Workshops ,\\nMay. 2022.\\n[8] A. Gupta, A. Pacchiano et al. , “Unpacking Reward Shaping: Understand-\\ning the Benefits of Reward Engineering on Sample Complexity,” in Proc.\\nConf. on Neural Info. Proces. Syst. (NeurIPS) , Nov. 2022.\\n[9] A. Mott, D. Zoran et al. , “Towards Interpretable Reinforcement Learning\\nUsing Attention Augmented Agents,” in Proc. Conf. on Neural Info.\\nProces. Syst. (NeurIPS) , Dec. 2019.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c1fa8a88-0655-4689-b136-b0ff9c3fc4db', embedding=None, metadata={'page_label': '', 'file_name': '2307.01658v1.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/2307.01658v1.pdf', 'file_type': 'application/pdf', 'file_size': 666755, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[10] ETSI. GS ZSM 002 V1.1.1, “Zero-touch network and Service Manage-\\nment (ZSM),” Reference Architecture. Group specification (gs), 08 , 2019.\\n[11] F. Rezazadeh, H. Chergui, L. Alonso, and C. Verikoukis, “Continuous\\nMulti-objective Zero-touch Network Slicing via Twin Delayed DDPG\\nand OpenAI Gym,” in Proc. IEEE Glob. Commun. Conf. , Dec. 2020.\\n[12] S. M. Lundberg and S. Lee, “A Unified Approach to Interpreting Model\\nPredictions,” in Advances in Neural Information Processing Systems 30 ,\\n2017.\\n[13] O-RAN.WG1, “Slicing Architecture Technical Specification 9.0,” in O-\\nRAN Specifications , March 2023.\\n[14] F. Rezazadeh, L. Zanzi, F. Devoti, H. Chergui, X. Costa-Pérez, and\\nC. Verikoukis, “On the Specialization of FDRL Agents for Scalable\\nand Distributed 6G RAN Slicing Orchestration,” IEEE Transactions on\\nVehicular Technology , vol. 72, no. 3, pp. 3473–3487, 2022.\\n[15] Y . Hua, R. Li, Z. Zhao, X. Chen, and H. Zhang, “GAN-Powered\\nDeep Distributional Reinforcement Learning for Resource Management\\nin Network Slicing,” IEEE Jour. Selec. Areas Commun. , vol. 38, no. 2,\\npp. 334–349, Feb. 2020.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fb54bfad-47ff-43e5-984a-73d4fc015238', embedding=None, metadata={'page_label': '31866', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Received 20 February 2023, accepted 16 March 2023, date of publication 27 March 2023, date of current version 3 April 2023.\\nDigital Object Identifier 10.1 109/ACCESS.2023.3262138\\nMachine Learning Operations (MLOps):\\nOverview, Definition, and Architecture\\nDOMINIK KREUZBERGER1, NIKLAS KÜHL\\n1,2, AND SEBASTIAN HIRSCHL1\\n1IBM, 71139 Ehningen, Germany\\n2Information Systems and Human-Centric Artificial Intelligence, University of Bayreuth, 95447 Bayreuth, Germany\\nCorresponding author: Niklas Kühl (kuehl@uni-bayreuth.de)\\nThis work was supported in part by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Grant 491183248, and\\nin part by the Open Access Publishing Fund of the University of Bayreuth.\\nABSTRACT The final goal of all industrial machine learning (ML) projects is to develop ML products\\nand rapidly bring them into production. However, it is highly challenging to automate and operationalize\\nML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine\\nLearning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices,\\nsets of concepts, and development culture. However, MLOps is still a vague term and its consequences\\nfor researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research,\\nincluding a literature review, a tool review, and expert interviews. As a result of these investigations,\\nwe contribute to the body of knowledge by providing an aggregated overview of the necessary principles,\\ncomponents, and roles, as well as the associated architecture and workflows. Furthermore, we provide a\\ncomprehensive definition of MLOps and highlight open challenges in the field. Finally, this work provides\\nguidance for ML researchers and practitioners who want to automate and operate their ML products with a\\ndesignated set of technologies.\\nINDEX TERMS CI/CD, DevOps, machine learning, MLOps, operations, workflow orchestration.\\nI. INTRODUCTION\\nMachine Learning (ML) has become an important technique\\nto leverage the potential of data and allows businesses to\\nbe more innovative [1], efficient [2], and sustainable [3].\\nHowever, the success of many productive ML applications\\nin real-world settings falls short of expectations [4]. A large\\nnumber of ML projects fail—with many ML proofs of con-\\ncept never progressing as far as production [5]. From a\\nresearch perspective, this does not come as a surprise as the\\nML community has focused extensively on the building of\\nML models, but not on (a) building production-ready ML\\nproducts and (b) providing the necessary coordination of\\nthe resulting, often complex ML system components and\\ninfrastructure, including the roles required to automate and\\noperate an ML system in a real-world setting [6]. For instance,\\nin many industrial applications, data scientists still manage\\nThe associate editor coordinating the review of this manuscript and\\napproving it for publication was Alberto Cano\\n .ML workflows manually to a great extent, resulting in many\\nissues during the operations of the respective ML solution [7].\\nTo address these issues, the goal of this work is to examine\\nhow manual ML processes can be automated and operational-\\nized so that more ML proofs of concept can be brought into\\nproduction. In this work, we explore the emerging ML engi-\\nneering practice ‘‘Machine Learning Operations’’—MLOps\\nfor short—precisely addressing the issue of designing and\\nmaintaining productive ML. We take a holistic perspective to\\ngain a common understanding of the involved components,\\nprinciples, roles, and architectures. While existing research\\nsheds some light on various specific aspects of MLOps,\\na holistic conceptualization, generalization, and clarification\\nof ML systems design are still missing. Different perspectives\\nand conceptions of the term ‘‘MLOps’’ might lead to misun-\\nderstandings and miscommunication, which, in turn, can lead\\nto errors in the overall setup of the entire ML system. Thus,\\nwe ask the research question:\\nRQ: What is MLOps?\\n31866This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 11, 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5835e26d-28e4-45b7-914d-957ff4739d1c', embedding=None, metadata={'page_label': '31867', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\nTo answer that question, we conduct a mixed-method\\nresearch endeavor to (a) identify important principles of\\nMLOps, (b) carve out functional core components, (c) high-\\nlight the roles necessary to successfully implement MLOps,\\nand (d) derive a general architecture for ML systems design.\\nIn combination, these insights result in a definition of\\nMLOps, which contributes to a common understanding of the\\nterm and related concepts.\\nTherefore, we hope to positively impact academic and\\npractical discussions by providing clear guidelines for pro-\\nfessionals and researchers alike with precise responsibilities.\\nThese insights can assist in allowing more proofs of concept\\nto make it into production by having fewer errors in the\\nsystem’s design and, finally, enabling more robust predictions\\nin real-world environments.\\nThe remainder of this article is structured as follows.\\nWe will first elaborate on the necessary foundations and\\nrelated work in the field. Next, we will give an overview of\\nthe utilized methodology, consisting of a literature review,\\na tool review, and an interview study. We then present the\\ninsights derived from the application of the methodology\\nand conceptualize these by providing a unifying definition.\\nWe conclude the paper with a short summary, limitations, and\\noutlook.\\nII. FOUNDATIONS OF DEVOPS\\nIn the past, different software process models and develop-\\nment methodologies surfaced in the field of software engi-\\nneering. Prominent examples include waterfall [8] and the\\nagile manifesto [9]. Those methodologies have similar aims,\\nnamely to deliver production-ready software products. A con-\\ncept called ‘‘DevOps’’ emerged in the years 2008/2009 and\\naims to reduce issues in software development [10], [11].\\nDevOps is more than a pure methodology and rather repre-\\nsents a paradigm addressing social and technical issues in\\norganizations engaged in software development. It has the\\ngoal of eliminating the gap between development and oper-\\nations and emphasizes collaboration, communication, and\\nknowledge sharing. DevOps promotes automation through\\nthe tactic of continuous integration, continuous delivery, and\\ncontinuous deployment (CI/CD), enabling fast, frequent, and\\nreliable releases. Moreover, it is designed to ensure con-\\ntinuous testing, quality assurance, continuous monitoring,\\nlogging, and feedback loops. Due to the commercialization\\nof DevOps, many DevOps tools are emerging, which can be\\ndifferentiated into six groups [12], [13]: collaboration and\\nknowledge sharing (e.g., Slack, Trello, GitLab wiki), source\\ncode management (e.g., GitHub, GitLab), build process (e.g.,\\nMaven), continuous integration (e.g., Jenkins, GitLab CI),\\ndeployment automation (e.g., Kubernetes, Docker), mon-\\nitoring and logging (e.g., Prometheus, Logstash). Cloud\\nenvironments are increasingly equipped with ready-to-use\\nDevOps tooling that is designed for cloud use, facilitating\\nthe efficient generation of value [14]. With this novel shift\\ntowards DevOps, developers need to care about what they\\ndevelop, as they need to operate it as well. As empirical\\nFIGURE 1. Overview of the methodology.\\nresults demonstrate, DevOps ensures better software qual-\\nity [15]. People in the industry, as well as academics, have\\ngained a wealth of experience in software engineering using\\nDevOps. This experience is now being used to automate and\\noperationalize ML.\\nIII. METHODOLOGY\\nTo derive insights from the academic knowledge base while\\nalso drawing upon the expertise of practitioners from the\\nfield, we apply a mixed-method approach, as depicted in\\nFigure 1. As a first step, we conduct a structured literature\\nreview [16], [17] to obtain an overview of relevant research.\\nFurthermore, we review relevant tooling support in the field\\nof MLOps to gain a better understanding of the technical\\ncomponents involved. Finally, we conduct semi-structured\\ninterviews [18], [19] with experts from different domains.\\nOn that basis, we conceptualize the term ‘‘MLOps’’ and elab-\\norate on our findings by synthesizing literature and interviews\\nin the next chapter (‘‘Results’’).\\nA. LITERATURE REVIEW\\nTo ensure that our results are based on scientific knowledge,\\nwe conduct a systematic literature review according to the\\nmethod of Webster and Watson [16] and Kitchenham et al.\\n[17]. After an initial exploratory search, we define our search\\nquery as follows: (((‘‘DevOps’’ OR ‘‘CICD’’ OR ‘‘Continuous\\nIntegration’’ OR ‘‘Continuous Delivery’’ OR ‘‘Continuous\\nDeployment’’) AND ‘‘Machine Learning’’) OR ‘‘MLOps’’ OR\\n‘‘CD4ML’’).\\nWe query the scientific databases of Google Scholar, Web\\nof Science, Science Direct, Scopus, and the Association for\\nInformation Systems eLibrary. It should be mentioned that\\nthe use of DevOps for ML, MLOps, and continuous prac-\\ntices in combination with ML is a relatively new field in\\nacademic literature. Thus, only a few peer-reviewed stud-\\nies are available at the time of this research. Neverthe-\\nless, to gain experience in this area, the search included\\nnon-peer-reviewed literature as well. The search was per-\\nformed in May 2021 and resulted in 1,864 retrieved articles.\\nOf those, we screened 194 papers in detail. From that group,\\nVOLUME 11, 2023 31867', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7adfb623-8b2a-4de9-ae20-89faadb9e2f2', embedding=None, metadata={'page_label': '31868', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\nTABLE 1. List of evaluated technologies.\\n27 articles were selected based on our inclusion and exclusion\\ncriteria (e.g., the term MLOps or DevOps and CI/CD in\\ncombination with ML was described in detail, the article\\nwas written in English, etc.). All 27 of these articles were\\npeer-reviewed.B. TOOL REVIEW\\nAfter going through 27 articles and eight interviews, vari-\\nous open-source tools, frameworks, and commercial cloud\\nML services were identified. These tools, frameworks, and\\nML services were reviewed to gain an understanding of the\\n31868 VOLUME 11, 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ecba89c-56a1-4fa0-b50c-9945e34a02fb', embedding=None, metadata={'page_label': '31869', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\nTABLE 2. List of interview partners.\\ntechnical components of which they consist. An overview of\\nthe identified tools is depicted in Table 1.\\nC. INTERVIEW STUDY\\nTo answer the research questions with insights from practice,\\nwe conduct semi-structured expert interviews according to\\nMyers and Newman [18]. One major aspect in the research\\ndesign of expert interviews is choosing an appropriate sample\\nsize [20]. We apply a theoretical sampling approach [21],\\nwhich allows us to choose experienced interview partners to\\nobtain high-quality data. Such data can provide meaningful\\ninsights with a limited number of interviews. To get an ade-\\nquate sample group and reliable insights, we use LinkedIn—a\\nsocial network for professionals—to identify experienced\\nML professionals with profound MLOps knowledge on a\\nglobal level. To gain insights from various perspectives,\\nwe choose interview partners from different organizations\\nand industries, different countries, and nationalities, as well\\nas different genders. Interviews are conducted until no new\\ncategories and concepts emerge in the analysis of the data.\\nAccording to Glaser and Strauss [21], this stage is called\\n‘‘theoretical saturation.’’ In total, we conduct eight interviews\\nwith experts (pseudonymized with α-θ), whose details are\\ndepicted in Table 2. All interviews are conducted between\\nJune and August 2021.With regard to the interview design, we prepare a semi-\\nstructured guide with several questions, documented as an\\ninterview script [18]. During the interviews, ‘‘soft ladder-\\ning’’ is used with ‘‘how’’ and ‘‘why’’ questions to probe\\nthe interviewees’ means-end chain [19]. This methodical\\napproach allowed us to gain additional insight into the\\nexperiences of the interviewees when required. All inter-\\nviews are recorded and then transcribed. To evaluate the\\ninterview transcripts, we use an open coding scheme [20].\\nThe open coding process allows the data to be broken\\ndown in an analytical manner so that conceptually sim-\\nilar topics can be grouped into categories and subcate-\\ngories. These categories are called ‘‘codes’’. Concepts were\\nidentified when they appeared multiple times in different\\ninterviews [21].\\nIV. RESULTS\\nWe apply the described methodology and structure our result-\\ning insights into a presentation of important principles, their\\nresulting instantiation as components, the description of nec-\\nessary roles, as well as a suggestion for the architecture and\\nworkflow resulting from the combination of these aspects.\\nFinally, we derive the conceptualization of the term and pro-\\nvide a definition of MLOps.\\nVOLUME 11, 2023 31869', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='961eb4df-2257-46b6-a084-3d24028e2dd5', embedding=None, metadata={'page_label': '31870', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\nFIGURE 2. Implementation of principles within technical components.\\nA. PRINCIPLES\\nA principle is viewed as a general or basic truth, a value, or a\\nguide for behavior. In the context of MLOps, a principle is\\na guide to how things should be realized in MLOps and is\\nclosely related to the term ‘‘best practices’’ from the profes-\\nsional sector. Based on the outlined methodology, we iden-\\ntified nine principles required to realize MLOps. Figure 2\\nprovides an illustration of these principles and links them to\\nthe components with which they are associated.\\nP1 CI/CD automation. CI/CD automation provides con-\\ntinuous integration, continuous delivery, and continuous\\ndeployment. It carries out the build, test, delivery, and deploy\\nsteps. It provides fast feedback to developers regarding the\\nsuccess or failure of certain steps, thus increasing the over-\\nall productivity. CI/CD puts ideas of DevOps into prac-\\ntice. Therefore, CI/CD can be seen as a DevOps tactic [6],\\n[7], [22], [23], [α ,β,θ].\\nP2 Workflow orchestration. Workflow orchestration\\ncoordinates the tasks of an ML workflow pipeline according\\nto directed acyclic graphs (DAGs). DAGs define the task\\nexecution order by considering relationships and dependen-\\ncies [7], [24], [25], [26] [α ,β,γ,δ,ζ,η].\\nP3 Reproducibility. Reproducibility is the ability to\\nreproduce an ML experiment and obtain the exact same\\nresults [23], [27] [α ,β,δ,ε,η].\\nP4 Versioning. Versioning ensures the versioning of data,\\nmodel, and code to enable not only reproducibility, but also\\ntraceability (for compliance and auditing reasons) [23], [27]\\n[α,β,δ,ε,η].\\nP5 Collaboration. Collaboration ensures the possibility to\\nwork collaboratively on data, model, and code. Besides the\\ntechnical aspect, this principle emphasizes a collaborative and\\ncommunicative work culture aiming to reduce domain silos\\nbetween different roles [7], [25], [27], [α ,δ,θ].\\nP6 Continuous ML training & evaluation. Continuous\\ntraining (CT) means periodic retraining of the ML model\\nbased on new feature data. Continuous training is enabled\\nthrough the support of a monitoring component, a feedback\\nloop, and an automated ML workflow pipeline. Continu-\\nous training always includes an evaluation run to assess the\\nchange in model quality [23], [24], [28], [29], [β, δ,η,θ].\\nIn general, to manage costs of retraining, it should be care-\\nfully considered, which update frequency is necessary for theuse case (e.g., daily vs. weekly). A powerful tool to decrease\\ncost of retraining is the use of online learning in large scale\\nweb applications, benefiting from iterative training steps\\ncompared to a full training data set. This way the model can\\nalso reflect recent impactful events like catastrophes. There\\nis a magnitude of online learning optimization algorithms\\navailable [30].\\nP7 ML metadata tracking/logging. Metadata is tracked\\nand logged for each orchestrated ML workflow task. Meta-\\ndata tracking and logging is required for each training job\\niteration (e.g., training date and time, duration, etc.), includ-\\ning the model specific metadata—e.g., used parameters and\\nthe resulting performance metrics, model lineage: data and\\ncode used—to ensure the full traceability of experiment\\nruns [6], [7], [31], [32], [α ,β,δ,ε,ζ,η,θ].\\nP8 Continuous monitoring. Continuous monitoring\\nimplies the periodic assessment of data, model, code, infras-\\ntructure resources, and model serving performance (e.g., pre-\\ndiction accuracy) to detect potential errors or changes that\\ninfluence the product quality [7], [23], [28], [32], [33], [α ,\\nβ,γ,δ,ε,ζ,η].\\nP9 Feedback loops. Multiple feedback loops are required\\nto integrate insights from the quality assessment step into\\nthe development or engineering process (e.g., a feedback\\nloop from the experimental model engineering stage to the\\nprevious feature engineering stage). Another feedback loop\\nis required from the monitoring component (e.g., observing\\nthe model serving performance) to the scheduler to enable the\\nretraining [7], [23], [24], [34], [35], [α ,β,δ,ζ,η,θ].\\nB. TECHNICAL COMPONENTS\\nAfter identifying the principles that need to be incorporated\\ninto MLOps, we now elaborate on the precise components\\nand implement them in the ML systems design. In the fol-\\nlowing, the components are listed and described in a generic\\nway with their essential functionalities. The references in\\nbrackets refer to the respective principles that the technical\\ncomponents are implementing.\\nC1 CI/CD Component (P1, P6, P9). The CI/CD com-\\nponent ensures continuous integration, continuous deliv-\\nery, and continuous deployment. It takes care of the\\nbuild, test, delivery, and deploy steps. It provides rapid\\nfeedback to developers regarding the success or failure\\nof certain steps, thus increasing the overall productivity\\n[6], [7], [22], [23], [24], [28], [α ,β,γ,ε,ζ,η]. Examples\\nare Jenkins [7], [24], and GitHub actions (η). For implement-\\ning an MLOps workflow, this means the automated linting,\\nassembly and registry of training inference and application\\ncode into a shippable format (e.g., Python Wheel) as well as\\nexecution of unit and integration test cases. This automation\\nshould be an idempotent process using dynamically assigned\\nresources from the CI/CD tool, that results in a binary or\\narchive-based package.\\nC2 Source Code Repository (P4, P5). The training, infer-\\nence and application source code is versioned in a repository.\\nIt allows multiple developers to commit and merge their\\n31870 VOLUME 11, 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b806d9c5-ffc7-480a-88f0-2ffa08d4ab30', embedding=None, metadata={'page_label': '31871', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\ncode [23], [24], [36], [37], [38] [α ,β,γ,ζ,θ]. Exam-\\nples include Bitbucket [39], [ζ], GitLab [24], [39], [ζ],\\nGitHub [37], [ζ, η], and Gitea [23].\\nC3 Workflow Orchestration Component (P2, P3, P6).\\nThe workflow orchestration component offers task orchestra-\\ntion of an ML workflow via directed acyclic graphs (DAGs).\\nThese graphs represent execution order and artifact usage of\\nsingle steps of the workflow. A workflow uses for example\\npackaged code artifacts in the respective process step, like\\nextracting data, training, inference or embedding of a model\\nbinary into an application [6], [7], [23], [26], [31], [α ,β,γ,δ,\\nε,ζ,η]. Examples include Apache Airflow [α ,ζ], Kubeflow\\nPipelines [ζ], Watson Studio Pipelines [γ ], Luigi [ζ], AWS\\nSageMaker Pipelines [β ], and Azure Pipelines [ε]. In theory,\\nCI/CD tools could also be used to schedule the triggering of\\nspecific tasks sequentially, however the complexity of data\\nengineering- or ML-pipeline tasks has increased the need for\\na tool specifically designed for the purpose of workflow or\\ntask orchestration. These workflow orchestration tools make\\nit easier to efficiently manage interrelated and interdependent\\ntasks, because they are specifically designed to manage com-\\nplex task chains [40].\\nC4 Feature Store System (P3, P4). A feature store system\\nensures central storage of commonly used features. It has two\\ndatabases configured: One database as an offline feature store\\nto serve features with normal latency for experimentation,\\nand one database as an online store to serve features with\\nlow latency for predictions in production [25], [28], [α ,β,\\nζ,ε,θ]. Examples include Google Feast [ζ], Amazon AWS\\nFeature Store [β ,ζ], Tecton.ai and Hopswork.ai [ζ]. This is\\nwhere most of the data for training ML models will come\\nfrom. Moreover, data can also come directly from any kind of\\ndata store. A feature store poses complex requirements, which\\nare highly dependent on the use case. Databases of it can be\\nhosted on on-premises infrastructure or in the cloud. How-\\never, scalability is typically realized with cloud infrastructure.\\nMost use cases have a read-heavy workload, combined with\\na batch or streaming-based ingestion pattern on (very) large\\ndata sets. Such high scalability can be achieved on distributed\\nfile systems [41], [42], or distributed databases [43], [44]\\ncombined with parallel and distributed data processing algo-\\nrithms (e.g., MapReduce or a more high-level API like\\nSpark).\\nC5 Model Training Infrastructure (P6). The model\\ntraining infrastructure provides the foundational computation\\nresources, e.g., CPUs, RAM, and GPUs. The provided infras-\\ntructure can be either distributed or non-distributed. In gen-\\neral, a scalable and distributed infrastructure is recommended\\n[7], [23], [27], [28], [32], [33], [37], [45], [46], [δ, ζ,η,θ].\\nExamples include local machines (not scalable) or cloud\\ncomputation [33] [η, θ], as well as non-distributed or dis-\\ntributed computation (several worker nodes) [7], [37]. Frame-\\nworks supporting computation are Kubernetes [η, θ] and\\nRed Hat OpenShift [γ ]. Typically, deep learning workloads\\n(training and inference) are matrix-multiplication-heavy andtherefore computation bound. GPUs are optimized towards\\nthis type of workload and should be the primary focus for\\ncompute node specification. In edge devices, where stor-\\nage and computation power are limited, Quantized Neural\\nNets with low-precision floating-point operations [47] in\\ncombination with pruning and Hofmann Coding should be\\ninvestigated [48].\\nC6 Model Registry (P3, P4). The model registry stores\\ncentrally the trained ML models together with their metadata.\\nIt has two main functionalities: storing the ML artifact and\\nstoring the ML metadata (see C7) [7], [24], [25], [34], [35],\\n[α,β,γ,ε,ζ,η,θ]. Advanced storage examples include\\nMLflow [α ,η,ζ], AWS SageMaker Model Registry [ζ],\\nMicrosoft Azure ML Model Registry [ζ], and Neptune.ai [α ].\\nSimple storage examples include Microsoft Azure Storage,\\nGoogle Cloud Storage, and Amazon AWS S3 [23].\\nC7 ML Metadata Stores (P4, P7). ML metadata stores\\nallow for the tracking of various kinds of metadata, e.g.,\\nfor each orchestrated ML workflow pipeline task. Another\\nmetadata store can be configured within the model registry for\\ntracking and logging the metadata of each training job (e.g.,\\ntraining date and time, duration, etc.), including the model\\nspecific metadata—e.g., used parameters and the resulting\\nperformance metrics, model lineage: data and code used [7],\\n[25], [31], [37], [49], [α ,β,δ,ζ,θ]. Examples include orches-\\ntrators with built-in metadata stores tracking each step of\\nexperiment pipelines [α ] such as Kubeflow Pipelines [α ,ζ],\\nAWS SageMaker Pipelines [α ,ζ], Azure ML, and IBM Wat-\\nson Studio [γ ]. MLflow provides an advanced metadata store\\nin combination with the model registry [6], [31].\\nC8 Model Serving Component (P1). The model serving\\ncomponent can be configured for different purposes. Exam-\\nples are online inference for real-time predictions or batch\\ninference for predictions using large volumes of input data.\\nThe serving can be provided, e.g., via a REST API. As a foun-\\ndational infrastructure layer, a scalable and distributed model\\nserving infrastructure is recommended [23], [27], [33], [37],\\n[39], [46], [α ,β,δ,ζ,η,θ]. One example of a model serving\\ncomponent configuration is the use of Kubernetes and Docker\\ntechnology to containerize the ML model, and leveraging a\\nPython web application framework like Flask [24] with an\\nAPI for serving [α ]. Other Kubernetes supported frameworks\\nare KServing of Kubeflow [α ], TensorFlow Serving, and\\nSeldion.io serving [27]. Inferencing could also be realized\\nwith Apache Spark for batch predictions [θ]. Examples of\\ncloud services include Microsoft Azure ML REST API [ε],\\nAWS SageMaker Endpoints [α ,β], IBM Watson Studio [γ ],\\nand Google Vertex AI prediction service [δ]. The actual\\ndeployment of the model depends on the use case and\\nfalls typically into one of these categories: real-time, batch,\\nor serverless inference. Real-time inference can be achieved\\nby hosting the model in a RESTful web-service, batch infer-\\nence can be an idempotent MapReduce workflow, and server-\\nless inference is used when cost-efficient and scalable serving\\nis required.\\nVOLUME 11, 2023 31871', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c2295b7c-98e4-4ac2-b280-bfd3fc2d784f', embedding=None, metadata={'page_label': '31872', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\nC9 Monitoring Component (P8, P9). The monitoring\\ncomponent takes care of the continuous monitoring of the\\nmodel serving performance (e.g., prediction accuracy). Addi-\\ntionally, monitoring of the ML infrastructure, CI/CD, and\\norchestration are required [7], [23], [24], [28], [32], [33],\\n[50], [α ,ζ,η,θ]. Examples include Prometheus with Grafana\\n[η,ζ], ELK stack (Elasticsearch, Logstash, and Kibana) [α ,\\nη,ζ], and simply TensorBoard [θ]. Examples with built-in\\nmonitoring capabilities are Kubeflow [θ], MLflow [η], and\\nAWS SageMaker model monitor or cloud watch [ζ].\\nC. ROLES\\nAfter describing the principles and their resulting instanti-\\nation of components, we identify necessary roles in order\\nto realize MLOps in the following. MLOps is an interdis-\\nciplinary group process, and the interplay of different roles\\nis crucial to design, manage, automate, and operate an ML\\nsystem in production. In the following, every role, its purpose,\\nand related tasks are briefly described:\\nR1 Business Stakeholder (similar roles: Product Owner,\\nProject Manager). The business stakeholder defines the\\nbusiness goal to be achieved with ML and takes care of\\nthe communication side of the business, e.g., presenting\\nthe return on investment (ROI) generated with an ML\\nproduct [7], [24], [45] [α ,β,δ,θ].\\nR2 Solution Architect (similar role: IT Architect).\\nThe solution architect designs the architecture and defines\\nthe technologies to be used, following a thorough\\nevaluation [7], [24], [α ,ζ].\\nR3 Data Scientist (similar roles: ML Specialist,\\nML Developer). The data scientist translates the business\\nproblem into an ML problem and takes care of the model\\nengineering, including the selection of the best-performing\\nalgorithm and hyperparameters [7], [25], [32], [33], [α ,β,γ,\\nδ,ε,ζ,η,θ].\\nR4 Data Engineer (similar role: DataOps Engineer).\\nThe data engineer builds up and manages data and fea-\\nture engineering pipelines. Moreover, this role ensures\\nproper data ingestion to the databases of the feature store\\nsystem [25], [26], [32], [α ,β,γ,δ,ε,ζ,η,θ].\\nR5 Software Engineer. The software engineer applies\\nsoftware design patterns, widely accepted coding guidelines,\\nand best practices to turn the raw ML problem into a well-\\nengineered product [32], [α ,γ].\\nR6 DevOps Engineer. The DevOps engineer bridges\\nthe gap between development and operations and ensures\\nproper CI/CD automation, ML workflow orchestra-\\ntion, model deployment to production, and monitoring\\n[7], [22], [25], [51], [α ,β,γ,ε,ζ,η,θ].\\nR7 ML Engineer/MLOps Engineer. The ML engineer\\nor MLOps engineer combines aspects of several roles and\\nthus has cross-domain knowledge. This role incorporates\\nskills from data scientists, data engineers, software engineers,\\nDevOps engineers, and backend engineers (see Figure 3).\\nThis cross-domain role builds up and operates the ML infras-\\ntructure, manages the automated ML workflow pipelines\\nFIGURE 3. Roles and their intersections contributing to the MLOps\\nparadigm.\\nand model deployment to production, and monitors both the\\nmodel and the ML infrastructure [7], [24], [25], [32], [α ,β,\\nγ,δ,ε,ζ,η,θ].\\nV. ARCHITECTURE AND WORKFLOW\\nOn the basis of the identified principles, components, and\\nroles, we derive a generalized MLOps end-to-end architecture\\nto give ML researchers and practitioners proper guidance.\\nIt is depicted in Figure 4. Additionally, we depict the work-\\nflows, i.e., the sequence in which the different tasks are\\nexecuted in the different stages. The artifact was designed to\\nbe technology-agnostic. Therefore, ML researchers and prac-\\ntitioners can choose the best-fitting technologies and frame-\\nworks for their needs. This means the MLOps process and\\ncomponents can either be built out of ‘‘best-of-breed’’ open-\\nsource tools, but also with enterprise solutions. Also, a mix\\nand match combination of enterprise and open-source tools\\nto realize MLOps is possible. Enterprise softwares / cloud\\nservices often allow the connection to open-source tools via\\ntheir APIs and vice versa. Thus, it should be considered to\\nhave a look at newest developments as the open source tool\\nmarket is growing rapidly. There are frequently new options\\nfor combinations possible. However, there are certainly also\\nsome constraints when it comes to API interfaces connections\\nand combinations. In general, it is hard to say which technolo-\\ngies are good to combine and which aren’t. However, with the\\nnewly introduced examples of applications and the precise\\nmentioning of tools, we demonstrate possible combinations.\\nAs depicted in Figure 4, we illustrate an end-to-end pro-\\ncess, from MLOps product initiation to the model serv-\\ning. It includes (A) the MLOps product initiation steps;\\n(B) the feature engineering pipeline, including the data\\ningestion to the feature store; (C) the experimentation; and\\n(D) the automated ML workflow pipeline up to the model\\nserving.\\n31872 VOLUME 11, 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aeeca6ab-eec5-434b-b256-95c4442255d7', embedding=None, metadata={'page_label': '31873', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\nFIGURE 4. End-to-end MLOps architecture and workflow with functional components and roles.\\n(A) MLOps product initiation. (1) The business stake-\\nholder (R1) analyzes the business and identifies a potential\\nbusiness problem that can be solved using ML. (2) The\\nsolution architect (R2) defines the architecture design for the\\noverall ML system, and decides on the technologies to be\\nused after a thorough evaluation. (3) The data scientist (R3)\\nderives an ML problem—such as whether regression or clas-\\nsification should be used—from the business goal. (4) Thedata engineer (R4) and the data scientist (R3) work together\\nto understand which data is required to solve the problem.\\n(5) Once the answers are clarified, the data engineer (R4) and\\ndata scientist (R3) collaborate to locate the raw data sources\\nfor the initial data analysis. They check the distribution, and\\nquality of the data, as well as performing validation checks.\\nFurthermore, they ensure that the incoming data from the\\ndata sources is labeled, meaning that a target attribute is\\nVOLUME 11, 2023 31873', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b429f046-b20c-45ce-9cde-194b57ad81b4', embedding=None, metadata={'page_label': '31874', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\nknown, as this is a mandatory requirement for supervised ML.\\nIn this example, the data sources already had labeled data\\navailable as the labeling step was covered during an upstream\\nprocess.\\n(B1) Requirements for feature engineering pipeline.\\nThe features are the relevant attributes required for model\\ntraining. After the initial understanding of the raw data and\\nthe initial data analysis, the fundamental requirements for the\\nfeature engineering pipeline are defined, as follows: (6) The\\ndata engineer (R4) defines the data transformation rules (nor-\\nmalization, aggregations) and cleaning rules to bring the data\\ninto a usable format. (7) The data scientist (R3) and data\\nengineer (R4) together define the feature engineering rules,\\nsuch as the calculation of new and more advanced features\\nbased on other features. These initially defined rules must be\\niteratively adjusted by the data scientist (R3) either based on\\nthe feedback coming from the experimental model engineer-\\ning stage or from the monitoring component observing the\\nmodel performance.\\n(B2) Feature engineering pipeline. The initially defined\\nrequirements for the feature engineering pipeline are taken\\nby the data engineer (R4) and software engineer (R5) as a\\nstarting point to build up the prototype of the feature engi-\\nneering pipeline. The initially defined requirements and rules\\nare updated according to the iterative feedback coming either\\nfrom the experimental model engineering stage or from the\\nmonitoring component observing the model’s performance in\\nproduction.\\nAs a foundational requirement, the data engineer (R4)\\ndefines the code required for the CI/CD (C1) and orches-\\ntration component (C3) to ensure the task orchestration of\\nthe feature engineering pipeline. This role also defines the\\nunderlying infrastructure resource configuration. (8) First,\\nthe feature engineering pipeline connects to the raw data,\\nwhich can be (for instance) streaming data, static batch data,\\nor data from any cloud storage. (9) The data will be extracted\\nfrom the data sources. (10) The data preprocessing begins\\nwith data transformation and cleaning tasks. The transforma-\\ntion rule artifact defined in the requirement gathering stage\\nserves as input for this task, and the main aim of this task\\nis to bring the data into a usable format. These transforma-\\ntion rules are continuously improved based on the feedback.\\n(11) The feature engineering task calculates new and more\\nadvanced features based on other features. The predefined\\nfeature engineering rules serve as input for this task. These\\nfeature engineering rules are continuously improved based\\non the feedback. (12) Lastly, a data ingestion job loads\\nbatch or streaming data into the feature store system (C4).\\nThe target can either be the offline or online database (or\\nany kind of data store). An example of the implementation\\nof an entire feature engineering pipeline can be found in\\nEsmaeilzadeh et al. [52], who implemented an NLP pipeline\\nwith Apache Spark. As another example, Xu [53] demon-\\nstrates how a financial institution may use Spark to process\\nand analyze large amounts of customer credit data, such ascredit history, income, and demographics. The data is then\\ntransformed and cleaned using Spark’s DataFrame and SQL\\nfunctionality, and various feature engineering techniques are\\napplied to create a set of relevant features for the credit risk\\nmodel. These features can be then passed through an ML\\npipeline, also implemented in Spark, to train and evaluate a\\npredictive model for assessing credit risk. In addition, Apache\\nKafka can be used for near real-time streaming data ingestion\\ninto the Spark-based feature engineering pipeline [54]. How-\\never, to some extent, a traditional ETL tool can be used to\\nbuild a feature engineering pipeline [55].\\n(C) Experimentation. Most tasks in the experimentation\\nstage are led by the data scientist (R3) including the initial\\nconfiguration of the hardware and runtime environment. The\\ndata scientist is supported by the software engineer (R5).\\n(13) The data scientist (R3) connects to the feature store\\nsystem (C4) for the data analysis. (Alternatively, the data\\nscientist (R3) can also connect to the raw data for an initial\\nanalysis.) In case of any required data adjustments, the data\\nscientist (R3) reports the required changes back to the data\\nengineering zone (feedback loop). (14) Then the preparation\\nand validation of the data coming from the feature store\\nsystem is required. This task also includes the train and test\\nsplit dataset creation. (15) The data scientist (R3) estimates\\nthe best-performing algorithm and hyperparameters, and the\\nmodel training is then triggered with the training data (C5).\\nThe software engineer (R5) supports the data scientist (R3)\\nin the creation of well-engineered model training code. (16)\\nDifferent model parameters are tested and validated interac-\\ntively during several rounds of model training. Once the per-\\nformance metrics indicate good results, the iterative training\\nstops. The best-performing model parameters are identified\\nvia parameter tuning. The model training task and model\\nvalidation task are then iteratively repeated; together, these\\ntasks can be called ‘‘model engineering.’’ The model engi-\\nneering aims to identify the best-performing algorithm and\\nhyperparameters for the model. (17) The data scientist (R3)\\nexports the model and commits the code to the repository.\\nAs a foundational requirement, either the DevOps engineer\\n(R6) or the ML engineer (R7) defines the code for the (C2)\\nautomated ML workflow pipeline and commits it to the repos-\\nitory. Once either the data scientist (R3) commits a new ML\\nmodel or the DevOps engineer (R6) and the ML engineer (R7)\\ncommits new ML workflow pipeline code to the repository,\\nthe CI/CD component (C1) detects the updated code and\\ntriggers automatically the CI/CD pipeline carrying out the\\nbuild, test, and delivery steps. The build step creates artifacts\\ncontaining the ML model and tasks of the ML workflow\\npipeline. The test step validates the ML model and ML work-\\nflow pipeline code. The delivery step pushes the versioned\\nartifact(s)—such as images—to the artifact store (e.g., image\\nregistry).\\nTypical technologies used for the experimentation step\\nare notebook-based solutions like the ones from Jupyter.\\nOne example of an industry case where ML experiments\\n31874 VOLUME 11, 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ae5dff8f-01c8-423b-b2a6-9b017c991c46', embedding=None, metadata={'page_label': '31875', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\nare performed with a notebook-based environment is in the\\nfield of natural language processing (NLP) [56]. A com-\\npany that provides NLP-based services such as sentiment\\nanalysis, text summarization, and named entity recognition,\\nmay use Jupyter notebooks to perform ML experiments on\\nlarge amounts of text data. The company’s data scientists use\\nJupyter notebooks to prepare the data. Then, they can train,\\nevaluate, and optimize different ML models, such as deep\\nlearning models, and test the results. To track the experiments\\nwith the textual data, i.e., the tracking of meta data and storing\\nthe resulting models, common-used solutions in combination\\nwith Jupyter are, among others, MLflow (i.e., Obeid [57]\\nfor assessing the risk of COVID-19 based on health records)\\nor Neptune.AI (i.e., Aljabri [58] for NLP-based fake news\\ndetection).\\n(D) Automated ML workflow pipeline. The DevOps\\nengineer (R6) and the ML engineer (R7) take care of the\\nmanagement of the automated ML workflow pipeline. They\\nalso manage the runtime environments, the underlying model\\ntraining infrastructure in the form of hardware resources\\nand frameworks supporting computation such as Kubernetes\\n(C5). The workflow orchestration component (C3) orches-\\ntrates the tasks of the automated ML workflow pipeline.\\nFor each task, the required artifacts (e.g., images) are pulled\\nfrom the artifact store (e.g., image registry). Each task can\\nbe executed via an isolated environment (e.g., containers).\\nFinally, the workflow orchestration component (C3) gathers\\nmetadata for each task in the form of logs, completion time,\\nand so on.\\nOnce the automated ML workflow pipeline is triggered,\\neach of the following tasks is managed automatically: (18)\\nautomated pulling of the versioned features from the fea-\\nture store systems (data extraction). Depending on the use\\ncase, features are extracted from either the offline or online\\ndatabase (or any kind of data store). (19) Automated data\\npreparation and validation; in addition, the train and test split\\nis defined automatically. (20) Automated final model training\\non new unseen data (versioned features). The algorithm and\\nhyperparameters are already predefined based on the settings\\nof the previous experimentation stage. The model is retrained\\nand refined. (21) Automated model evaluation and iterative\\nadjustments of hyperparameters are executed, if required.\\nOnce the performance metrics indicate good results, the auto-\\nmated iterative training stops. The automated model training\\ntask and the automated model validation task can be itera-\\ntively repeated until a good result has been achieved. (22) The\\ntrained model is then exported and (23) pushed to the model\\nregistry (C6), where it is stored e.g., as code or containerized\\ntogether with its associated configuration and environment\\nfiles.\\nFor all training job iterations, the ML metadata store (C7)\\nrecords metadata such as parameters to train the model and\\nthe resulting performance metrics. This also includes the\\ntracking and logging of the training job ID, training date\\nand time, duration, and sources of artifacts. Additionally, themodel specific metadata called ‘‘model lineage’’ combin-\\ning the lineage of data and code is tracked for each newly\\nregistered model. This includes the source and version of\\nthe feature data and model training code used to train the\\nmodel. Also, the model version and status (e.g., staging or\\nproduction-ready) is recorded.\\nOnce the status of a well-performing model is switched\\nfrom staging to production, it is automatically handed over\\nto the DevOps engineer or ML engineer for model deploy-\\nment. From there, the (24) CI/CD component (C1) triggers\\nthe continuous deployment pipeline. The production-ready\\nML model and the model serving code are pulled (initially\\nprepared by the software engineer (R5)). The continuous\\ndeployment pipeline carries out the build and test step of\\nthe ML model and serving code and deploys the model for\\nproduction serving. The (25) model serving component (C8)\\nmakes predictions on new, unseen data coming from the\\nfeature store system (C4). This component can be designed\\nby the software engineer (R5) as online inference for real-\\ntime predictions or as batch inference for predictions con-\\ncerning large volumes of input data. For real-time predictions,\\nfeatures must come from the online database (low latency),\\nwhereas for batch predictions, features can be served from\\nthe offline database (normal latency). Model-serving applica-\\ntions are often configured within a container and prediction\\nrequests are handled via a REST API. When deploying an\\nML/AI application, it’s a good practice to use A/B testing\\nto determine in a real-world scenario which model performs\\nbetter compared to another model, for example, deploying\\na ‘‘challenger model’’ in addition to an existing ‘‘champion\\nmodel’’ to find out which one performs better by collecting\\nfeedback, for example, when predicting hotel booking can-\\ncellations [61].\\nAs a foundational requirement, the ML engineer (R7)\\nmanages the model-serving computation infrastructure. The\\n(26) monitoring component (C9) observes continuously the\\nmodel-serving performance and infrastructure in real-time.\\nOnce a certain threshold is reached, such as detection of\\nlow prediction accuracy, the information is forwarded via\\nthe feedback loop. The (27) feedback loop is connected to\\nthe monitoring component (C9) and ensures fast and direct\\nfeedback allowing for more robust and improved predic-\\ntions. It enables continuous training, retraining, and improve-\\nment. With the support of the feedback loop, information is\\ntransferred from the model monitoring component to several\\nupstream receiver points, such as the experimental stage, data\\nengineering zone, and the scheduler (trigger). The feedback\\nto the experimental stage is taken forward by the data scientist\\nfor further model improvements. The feedback to the data\\nengineering zone allows for the adjustment of the features\\nprepared for the feature store system. Additionally, the detec-\\ntion of concept drifts as a feedback mechanism can enable\\n(28) continuous training. Concept drifts occur in real-world\\napplications when the input data changes over time e.g.,\\nwhen a sensor breaks. Decreased prediction accuracy due\\nVOLUME 11, 2023 31875', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5fd48b68-ec5b-42e6-b907-0270edabe078', embedding=None, metadata={'page_label': '31876', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\nto concept drift can be detected by a certain concept drift\\ndetection algorithm [59]. Once the model-monitoring com-\\nponent (C9) detects a drift in the data [60], the information is\\nforwarded to the scheduler, which then triggers the automated\\nML workflow pipeline for retraining (continuous training).\\nAs explained, a change in adequacy of the deployed model\\ncan be detected using distribution comparisons to identify\\ndrift. Retraining is not only triggered automatically when\\na statistical threshold is reached; it can also be triggered\\nwhen new feature data is available, or it can be scheduled\\nperiodically.\\nTypical technologies supporting the automated ML work-\\nflow pipeline are, among others, Apache Airflow, Kubeflow\\nPipelines, IBM Watson Studio Pipelines, or SageMaker\\nPipelines. One example of an industry use case for an auto-\\nmated machine learning workflow pipeline using Airflow is\\nin the field of online advertising [62]. A company may use\\nAirflow to automate the process of training and deploying\\nmachine learning models for ad targeting and optimization.\\nThe pipeline starts by extracting, transforming, and loading\\nlarge amounts of data from various sources, such as website\\nclickstream data, user demographics, and campaign perfor-\\nmance data. This data is then passed through a series of\\npreprocessing and feature engineering steps, implemented\\nas Airflow operators. Next, different machine learning mod-\\nels are trained and evaluated on the processed data, also\\nusing Airflow operators. The best-performing model is then\\ndeployed to a production environment, where it is used to\\nmake real-time ad targeting decisions. In this case, Apache\\nAirflow is used to automate the entire process, including\\nscheduling, monitoring, and re-running failed tasks.\\nVI. CONCEPTUALIZATON\\nWith the findings at hand, we conceptualize the literature\\nand interviews. It becomes obvious that the term MLOps is\\npositioned at the intersection of machine learning, software\\nengineering, DevOps, and data engineering (see Figure 5).\\nWe define MLOps as follows:\\nMLOps (Machine Learning Operations) is a paradigm,\\nincluding aspects like best practices, sets of concepts, as\\nwell as a development culture when it comes to the end-to-\\nend conceptualization, implementation, monitoring, deploy-\\nment, and scalability of machine learning products. Most\\nof all, it is an engineering practice that leverages three\\ncontributing disciplines: machine learning, software engi-\\nneering (especially DevOps), and data engineering. MLOps\\nis aimed at productionizing machine learning systems by\\nbridging the gap between development (Dev) and operations\\n(Ops). Essentially, MLOps aims to facilitate the creation of\\nmachine learning products by leveraging these principles:\\nCI/CD automation, workflow orchestration, reproducibility;\\nversioning of data, model, and code; collaboration; con-\\ntinuous ML training and evaluation; ML metadata track-\\ning and logging; continuous monitoring; and feedback\\nloops.\\nFIGURE 5. Intersection of disciplines of the MLOps paradigm.\\nVII. OPEN CHALLENGES\\nSeveral challenges for adopting MLOps have been identified\\nafter conducting the literature review, tool review, and inter-\\nview study. These open challenges have been organized into\\nthe categories of organizational, ML system, and operational\\nchallenges.\\nA. ORGANIZATIONAL CHALLENGES\\nThe mindset and culture of data science practice is a typical\\nchallenge in organizational settings [63]. As our insights\\nfrom literature and interviews show, to successfully develop\\nand run ML products, there needs to be a culture shift\\naway from model-driven machine learning toward a product-\\noriented discipline [γ ]. The recent trend of data-centric AI\\nalso addresses this aspect by putting more focus on the data-\\nrelated aspects taking place prior to the ML model building.\\nEspecially the roles associated with these activities should\\nhave a product-focused perspective when designing ML\\nproducts [γ ]. A great number of skills and individual roles\\nare required for MLOps (β ). As our identified sources point\\nout, there is a lack of highly skilled experts for these roles—\\nespecially with regard to architects, data engineers, ML engi-\\nneers, and DevOps engineers [26], [32], [38], [α ,ε]. This is\\nrelated to the necessary education of future professionals—\\nas MLOps is typically not part of data science education [33]\\n[γ]. Posoldova [6] further stresses this aspect by remarking\\nthat students should not only learn about model creation, but\\nmust also learn about technologies and components necessary\\nto build functional ML products.\\nData scientists alone cannot achieve the goals of MLOps. A\\nmulti-disciplinary team is required [25], thus MLOps needs to\\nbe a group process [α ]. This is often hindered because teams\\nwork in silos rather than in cooperative setups [α ]. Addition-\\nally, different knowledge levels and specialized terminolo-\\ngies make communication difficult. To lay the foundations\\nfor more fruitful setups, the respective decision-makers\\n31876 VOLUME 11, 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='69122f45-3a3c-419e-bd19-92d8793f4ba1', embedding=None, metadata={'page_label': '31877', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\nneed to be convinced that an increased MLOps maturity\\nand a product-focused mindset will yield clear business\\nimprovements [γ ].\\nB. ML SYSTEM CHALLENGES\\nA major challenge with regard to MLOps systems is design-\\ning for fluctuating demand, especially in relation to the\\nprocess of ML training [33]. This stems from potentially\\nvoluminous and varying data [28], which makes it difficult\\nto precisely estimate the necessary infrastructure resources\\n(CPU, RAM, and GPU) and requires a high level of flexibility\\nin terms of scalability of the infrastructure [7], [33], [δ].\\nC. OPERATIONAL CHALLENGES\\nIn productive settings, it is challenging to operate ML\\nmanually due to different stacks of software and hardware\\ncomponents and their interplay as well as the selection of\\nboth ([64], [65]. Therefore, robust automation is required\\n[24], [33]. Also, a constant incoming stream of new data\\nforces retraining capabilities. This is a repetitive task which,\\nagain, requires a high level of automation [66], [θ]. These\\nrepetitive tasks yield a large number of artifacts that require\\na strong governance [27], [32], [45] as well as versioning\\nof data, model, and code to ensure robustness and repro-\\nducibility [7], [32], [39]. Lastly, it is challenging to resolve\\na potential support request (e.g., by finding the root cause),\\nas many parties and components are involved. Failures can\\nbe a combination of ML infrastructure and software within\\nthe MLOps stack [7].\\nVIII. CONCLUSION\\nWith the increase of data availability and analytical capabil-\\nities, coupled with the constant pressure to innovate, more\\nmachine learning products than ever are being developed.\\nHowever, only a small number of these proofs of concept\\nprogress into deployment and production. Furthermore, the\\nacademic space has focused intensively on machine learning\\nmodel building and benchmarking, but too little on operating\\ncomplex machine learning systems in real-world scenarios.\\nIn the real world, we observe data scientists still managing\\nML workflows manually to a great extent. The paradigm\\nof Machine Learning Operations (MLOps) addresses these\\nchallenges. In this work, we shed more light on MLOps.\\nBy conducting a mixed-method study analyzing existing\\nliterature and tools, as well as interviewing eight experts\\nfrom the field, we uncover four main aspects of MLOps: its\\nprinciples, components, roles, and architecture. From these\\naspects, we infer a holistic definition. The results support a\\ncommon understanding of the term MLOps and its associated\\nconcepts, and will hopefully assist researchers and profes-\\nsionals in setting up successful ML products in the future.\\nREFERENCES\\n[1] M. Aykol, P. Herring, and A. Anapolsky, ‘‘Machine learning for continuous\\ninnovation in battery technologies,’’ Nature Rev. Mater., vol. 5, no. 10,\\npp. 725–727, Jun. 2020.[2] M. K. Gourisaria, R. Agrawal, G. M. Harshvardhan, M. Pandey, and\\nS. S. Rautaray, ‘‘Application of machine learning in industry 4.0,’’ in\\nMachine Learning: Theoretical Foundations and Practical Applications.\\nCham, Switzerland: Springer, 2021, pp. 57–87.\\n[3] A. D. L. Heras, A. Luque-Sendra, and F. Zamora-Polo, ‘‘Machine learning\\ntechnologies for sustainability in smart cities in the post-COVID era,’’\\nSustainability, vol. 12, no. 22, p. 9320, Nov. 2020.\\n[4] R. Kocielnik, S. Amershi, and P. N. Bennett, ‘‘Will you accept an imperfect\\nAI?: Exploring designs for adjusting end-user expectations of AI systems,’’\\ninProc. CHI Conf. Hum. Factors Comput. Syst., May 2019, pp. 1–14.\\n[5] R. van der Meulen and T. McCall. (2018). Gartner Says Nearly Half\\nof CIOs Are Planning to Deploy Artificial Intelligence . Accessed:\\nDec. 4, 2021. [Online]. Available: https://www.gartner.com/en/newsroom/\\npress-releases/2018-02-13-gartner-says-nearly-half-of-cios-are-planning-\\nto-deploy-artificial-intelligence\\n[6] A. Posoldova, ‘‘Machine learning pipelines: From research to production,’’\\nIEEE Potentials, vol. 39, no. 6, pp. 38–42, Nov. 2020.\\n[7] L. E. Lwakatare, I. Crnkovic, E. Rånge, and J. Bosch, ‘‘From a data science\\ndriven process to a continuous delivery process for machine learning\\nsystems,’’ in Product-Focused Software Process Improvement (Lecture\\nNotes in Computer Science), vol. 12562. Springer, 2020, pp. 185–201, doi:\\n10.1007/978-3-030-64148-1_12.\\n[8] W. W. Royce, ‘‘Managing the development of large software systems,’’ in\\nProc. IEEE WESCON, Aug. 1970, pp. 1–9.\\n[9] K. Beck et al., ‘‘The agile manifesto,’’ 2001. [Online]. Available:\\nhttp://agilemanifesto.org/\\n[10] P. Debois. (2009). Patrick Debois Devopsdays Ghent. Accessed:\\nMar. 25, 2021. [Online]. Available: https://devopsdays.org/events/2019-\\nghent/speakers/patrick-debois/\\n[11] S. Mezak. (Jan. 25, 2018). The Origins of DevOps: What’s in a\\nName? DevOps.com. Accessed: Mar. 25, 2021. [Online]. Available:\\nhttps://devops.com/the-origins-of-devops-whats-in-a-name/\\n[12] L. Leite, C. Rocha, F. Kon, D. Milojicic, and P. Meirelles, ‘‘A survey of\\nDevOps concepts and challenges,’’ ACM Comput. Surv., vol. 52, no. 6,\\npp. 1–35, Nov. 2020, doi: 10.1145/3359981.\\n[13] R. W. Macarthy and J. M. Bass, ‘‘An empirical taxonomy of DevOps in\\npractice,’’ in Proc. 46th Euromicro Conf. Softw. Eng. Adv. Appl. (SEAA),\\nAug. 2020, pp. 221–228, doi: 10.1109/SEAA51224.2020.00046.\\n[14] M. Rütz, ‘‘DEVOPS: A systematic literature review,’’ Inf. Softw. Technol.,\\nFH-Wedel, Aug. 2019. [Online]. Available: https://www.researchgate.\\nnet/publication/335243102_DEVOPS_A_SYSTEMATIC_LITERATURE\\n_REVIEW\\n[15] P. Perera, R. Silva, and I. Perera, ‘‘Improve software quality through\\npracticing DevOps,’’ in Proc. 17th Int. Conf. Adv. ICT Emerg. Regions\\n(ICTer), Sep. 2017, pp. 1–6.\\n[16] J. Webster and R. Watson, ‘‘Analyzing the past to prepare for the\\nfuture: Writing a literature review,’’ MIS Quart., vol. 26, no. 2,\\npp. 8–23, 2002. [Online]. Available: https://www.jstor.org/stable/4132319,\\ndoi: 10.1.1.104.6570.\\n[17] B. Kitchenham, O. P. Brereton, D. Budgen, M. Turner, J. Bailey, and\\nS. Linkman, ‘‘Systematic literature reviews in software engineering—A\\nsystematic literature review,’’ Inf. Softw. Technol., vol. 51, no. 1, pp. 7–15,\\nJan. 2009, doi: 10.1016/j.infsof.2008.09.009.\\n[18] M. D. Myers and M. Newman, ‘‘The qualitative interview in IS research:\\nExamining the craft,’’ Inf. Org., vol. 17, no. 1, pp. 2–26, Jan. 2007, doi:\\n10.1016/j.infoandorg.2006.11.001.\\n[19] U. Schultze and M. Avital, ‘‘Designing interviews to generate rich data\\nfor information systems research,’’ Inf. Org., vol. 21, no. 1, pp. 1–16,\\nJan. 2011, doi: 10.1016/j.infoandorg.2010.11.001.\\n[20] J. M. Corbin and A. Strauss, ‘‘Grounded theory research: Procedures,\\ncanons, and evaluative criteria,’’ Qualitative Sociol., vol. 13, no. 1,\\npp. 3–21, 1990, doi: 10.1007/BF00988593.\\n[21] B. Glaser and A. Strauss, The Discovery of Grounded Theory: Strate-\\ngies for Qualitative Research. London, U.K.: Aldine, 1967, doi:\\n10.4324/9780203793206.\\n[22] T. Granlund, A. Kopponen, V. Stirbu, L. Myllyaho, and T. Mikkonen,\\n‘‘MLOps challenges in multi-organization setup: Experiences from two\\nreal-world cases,’’ 2021, arXiv:2103.08937.\\n[23] Y. Zhou, Y. Yu, and B. Ding, ‘‘Towards MLOps: A case study of ML\\npipeline platform,’’ in Proc. Int. Conf. Artif. Intell. Comput. Eng. (ICAICE) ,\\nOct. 2020, pp. 494–500, doi: 10.1109/ICAICE51518.2020.00102.\\nVOLUME 11, 2023 31877', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d2296d4-b12d-4c62-890d-6edc97781822', embedding=None, metadata={'page_label': '31878', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\n[24] I. Karamitsos, S. Albarhami, and C. Apostolopoulos, ‘‘Applying devops\\npractices of continuous automation for machine learning,’’ Information,\\nvol. 11, no. 7, pp. 1–15, 2020, doi: 10.3390/info11070363.\\n[25] A. Goyal, ‘‘MLOps machine learning operations,’’ Int. J. Inf.\\nTechnol. Insights Transformations, vol. 4, no. 2, 2020. Accessed:\\nApr. 15, 2021. [Online]. Available: http://technology.eurekajournals.com/\\nindex.php/IJITIT/article/view/655\\n[26] D. A. Tamburri, ‘‘Sustainable MLOps: Trends and challenges,’’ in Proc.\\n22nd Int. Symp. Symbolic Numeric Algorithms Sci. Comput. (SYNASC),\\nSep. 2020, pp. 17–23, doi: 10.1109/SYNASC51798.2020.00015.\\n[27] O. Spjuth, J. Frid, and A. Hellander, ‘‘The machine learning life\\ncycle and the cloud: Implications for drug discovery,’’ Expert Opin-\\nion Drug Discovery, vol. 16, no. 9, pp. 1071–1079, 2021, doi:\\n10.1080/17460441.2021.1932812.\\n[28] B. Derakhshan, A. R. Mahdiraji, T. Rabl, and V. Markl, ‘‘Continuous\\ndeployment of machine learning pipelines,’’ in Proc. EDBT, Mar. 2019,\\npp. 397–408, doi: 10.5441/002/edbt.2019.35.\\n[29] R. R. Karn, P. Kudva, and I. A. M. Elfadel, ‘‘Dynamic autoselection and\\nautotuning of machine learning models for cloud network analytics,’’ IEEE\\nTrans. Parallel Distrib. Syst., vol. 30, no. 5, pp. 1052–1064, May 2019, doi:\\n10.1109/TPDS.2018.2876844.\\n[30] S. Shalev-Shwartz, ‘‘Online learning and online convex optimization,’’\\nFound. Trends Mach. Learn., vol. 4, no. 2, pp. 107–194, 2012.\\n[31] A. Molner Domenech and A. Guillén, ‘‘Ml-experiment: A Python frame-\\nwork for reproducible data science,’’ J. Phys., Conf. Ser., vol. 1603, no. 1,\\nSep. 2020, Art. no. 012025, doi: 10.1088/1742-6596/1603/1/012025.\\n[32] S. Makinen, H. Skogstrom, E. Laaksonen, and T. Mikkonen, ‘‘Who needs\\nMLOps: What data scientists seek to accomplish and how can MLOps\\nhelp?’’ in Proc. IEEE/ACM 1st Workshop AI Eng. Softw. Eng. AI (WAIN),\\nMay 2021, pp. 109–112.\\n[33] L. C. Silva, F. R. Zagatti, B. S. Sette, L. N. dos Santos Silva,\\nD. Lucredio, D. F. Silva, and H. de Medeiros Caseli, ‘‘Benchmark-\\ning machine learning solutions in production,’’ in Proc. 19th IEEE\\nInt. Conf. Mach. Learn. Appl. (ICMLA), Dec. 2020, pp. 626–633, doi:\\n10.1109/ICMLA51294.2020.00104.\\n[34] A. Banerjee, C. C. Chen, C. C. Hung, X. Huang, Y. Wang, and\\nR. Chevesaran, ‘‘Challenges and experiences with MLOps for perfor-\\nmance diagnostics in hybrid-cloud enterprise software deployments,’’ in\\nProc. OpML USENIX Conf. Oper. Mach. Learn., 2020, pp. 7–9.\\n[35] B. Benni, M. Blay-Fornarino, S. Mosser, F. Precioso, and G. Jungbluth,\\n‘‘When DevOps meets meta-learning: A portfolio to rule them all,’’ in\\nProc. ACM/IEEE 22nd Int. Conf. Model Driven Eng. Lang. Syst. Com-\\npanion (MODELS-C), Sep. 2019, pp. 605–612, doi: 10.1109/MODELS-\\nC.2019.00092.\\n[36] C. Vuppalapati, A. Ilapakurti, K. Chillara, S. Kedari, and V. Mamidi,\\n‘‘Automating tiny ML intelligent sensors DevOPS using Microsoft azure,’’\\ninProc. IEEE Int. Conf. Big Data (Big Data), Dec. 2020, pp. 2375–2384,\\ndoi:10.1109/BigData50022.2020.9377755.\\n[37] Á. L. García, J. M. D. Lucas, M. Antonacci, W. Z. Castell, and\\nM. David, ‘‘A cloud-based framework for machine learning workloads\\nand applications,’’ IEEE Access, vol. 8, pp. 18681–18692, 2020, doi:\\n10.1109/ACCESS.2020.2964386.\\n[38] C. Wu, E. Haihong, and M. Song, ‘‘An automatic artificial intelligence\\ntraining platform based on kubernetes,’’ in Proc. 2nd Int. Conf. Big Data\\nEng. Technol., Jan. 2020, pp. 58–62, doi: 10.1145/3378904.3378921.\\n[39] G. Fursin, ‘‘Collective knowledge: Organizing research projects as a\\ndatabase of reusable components and portable workflows with common\\ninterfaces,’’ Phil. Trans. Roy. Soc. A, Math., Phys. Eng. Sci., vol. 379,\\nno. 2197, May 2021, Art. no. 20200211, doi: 10.1098/rsta.2020.0211.\\n[40] M. Schmitt, ‘‘Airflow vs. Luigi vs. Argo vs. MLFlow vs. KubeFlow,’’\\nTech. Rep., 2022. [Online]. Available: https://www.datarevenue.com/en-\\nblog/airflow-vs-luigi-vs-argo-vs-mlflow-vs-kubeflow\\n[41] K. Shvachko, H. Kuang, S. Radia, and R. Chansler, ‘‘The Hadoop dis-\\ntributed file system,’’ in Proc. IEEE 26th Symp. Mass Storage Syst. Tech-\\nnol. (MSST), May 2010, pp. 1–10.\\n[42] S. Ghemawat, H. Gobioff, and S.-T. Leung, ‘‘The Google file system,’’ in\\nProc. 19th ACM Symp. Operating Syst. Princ., Oct. 2003, pp. 29–43.\\n[43] A. Lakshman and P. Malik, ‘‘Cassandra: A decentralized structured stor-\\nage system,’’ ACM SIGOPS Oper. Syst. Rev., vol. 44, no. 2, pp. 35–40,\\nApr. 2010.\\n[44] J. C. Corbett, ‘‘Spanner: Google’s globally distributed database,’’ ACM\\nTrans. Comput. Syst. (TOCS), vol. 31, no. 3, pp. 1–22, 2013.[45] Y. Liu, Z. Ling, B. Huo, B. Wang, T. Chen, and E. Mouine, ‘‘Building\\na platform for machine learning operations from open source frame-\\nworks,’’ IFAC-PapersOnLine, vol. 53, no. 5, pp. 704–709, 2020, doi:\\n10.1016/j.ifacol.2021.04.161.\\n[46] G. S. Yoon, J. Han, S. Lee, and J. W. Kim, DevOps Portal Design for\\nSmartX AI Cluster Employing Cloud-Native Machine Learning Workflows,\\nvol. 47. Cham, Switzerland: Springer, 2020, doi: 10.1007/978-3-030-\\n39746-3_54.\\n[47] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, ‘‘Quan-\\ntized neural networks: Training neural networks with low precision weights\\nand activations,’’ J. Mach. Learn. Res., vol. 18, no. 1, pp. 6869–6898, 2017.\\n[48] S. Han, H. Mao, and W. J. Dally, ‘‘Deep compression: Compressing deep\\nneural networks with pruning, trained quantization and Huffman coding,’’\\n2015, arXiv:1510.00149.\\n[49] L. E. Lwakatare, I. Crnkovic, and J. Bosch, ‘‘DevOps for AI—\\nChallenges in development of AI-enabled applications,’’ in Proc. Int. Conf.\\nSoftw., Telecommun. Comput. Netw. (SoftCOM), Sep. 2020, pp. 1–6, doi:\\n10.23919/SoftCOM50211.2020.9238323.\\n[50] C. Renggli, L. Rimanic, N. M. Gürel, B. Karlaš, W. Wu, and C. Zhang,\\n‘‘A data quality-driven view of MLOps,’’ 2021, arXiv:2102.07750.\\n[51] W. J. van den Heuvel and D. A. Tamburri, Model-Driven ML-Ops for\\nIntelligent Enterprise Applications: Vision, Approaches and Challenges,\\nvol. 391. Cham, Switzerland: Springer, 2020, doi: 10.1007/978-3-030-\\n52306-0_11.\\n[52] A. Esmaeilzadeh, M. Heidari, R. Abdolazimi, P. Hajibabaee, and\\nM. Malekzadeh, ‘‘Efficient large scale NLP feature engineering with\\nApache spark,’’ in Proc. IEEE 12th Annu. Comput. Commun. Workshop\\nConf. (CCWC), Jan. 2022, pp. 274–280.\\n[53] J. Xu, ‘‘MLOps in the financial industry: Philosophy, practices, and tools,’’\\ninFuture and Fintech, the, Abcdi and Beyond. Singapore: World Scientific,\\n2022, p. 451, doi: 10.1142/9789811250903_0014.\\n[54] F. Carcillo, A. D. Pozzolo, Y.-A. L. Borgne, O. Caelen, Y. Mazzer, and\\nG. Bontempi. SCARFF?: A Scalable Framework for Streaming Credit\\nCard Fraud Detection With Spark 1. Accessed: Feb. 17, 2023. [Online].\\nAvailable: https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html\\n[55] J. Dhanalakshmi and N. Ayyanathan, ‘‘A dynamic web data extraction from\\nSRLDC (southern regional load dispatch centre) and feature engineering\\nusing ETL tool,’’ in Proc. 2nd Int. Conf. Artif. Intell., Adv. Appl. Springer,\\n2022, pp. 443–449, doi: 10.1007/978-981-16-6332-1_38.\\n[56] J. Foster and J. Wagner, ‘‘Naive Bayes versus BERT: Jupyter notebook\\nassignments for an introductory NLP course,’’ in Proc. 5th Workshop\\nTeaching (NLP), 2021, pp. 112–114.\\n[57] J. S. Obeid, M. Davis, M. Turner, S. M. Meystre, P. M. Heider,\\nE. C. O’Bryan, and L. A. Lenert, ‘‘An artificial intelligence approach to\\nCOVID-19 infection risk assessment in virtual visits: A case report,’’\\nJ. Amer. Med. Inform. Assoc., vol. 27, no. 8, pp. 1321–1325, Aug. 2020.\\n[58] M. Aljabri, D. M. Alomari, and M. Aboulnour, ‘‘Fake news detection\\nusing machine learning models,’’ in Proc. 14th Int. Conf. Comput. Intell.\\nCommun. Netw. (CICN), Dec. 2022, pp. 473–477.\\n[59] L. Baier, N. Kühl, and G. Satzger, ‘‘How to cope with change?—Preserving\\nvalidity of predictive services over time,’’ in Proc. Annu. Hawaii Int. Conf.\\nSyst. Sci., 2019, pp. 1–10.\\n[60] L. Baier, T. Schlör, J. Schöffer, and N. Kühl, ‘‘Detecting concept drift with\\nneural network model uncertainty,’’ 2021, arXiv:2107.01873.\\n[61] N. Antonio, A. de Almeida, and L. Nunes, ‘‘Predicting hotel bookings\\ncancellation with a machine learning classification model,’’ in Proc. 16th\\nIEEE Int. Conf. Mach. Learn. Appl. (ICMLA), Dec. 2017, pp. 1049–1054,\\ndoi:10.1109/ICMLA.2017.00-11.\\n[62] T. Cui, Y. Wang, and B. Namih, ‘‘Build an intelligent online marketing\\nsystem: An overview,’’ IEEE Internet Comput., vol. 23, no. 4, pp. 53–60,\\nJul. 2019.\\n[63] L. Baier and S. Seebacher, ‘‘Challenges in the Deployment and,’’ in Proc.\\n27th Eur. Conf. Inf. Syst. (ECIS), Stockholm, Sweden, Jun. 2019, pp. 1–15.\\n[Online]. Available: https://aisel.aisnet.org/ecis2019_rp/163\\n[64] P. Ruf, M. Madan, C. Reich, and D. Ould-Abdeslam, ‘‘Demystifying\\nMLOps and presenting a recipe for the selection of open-source tools,’’\\nAppl. Sci., vol. 11, no. 19, p. 8861, Sep. 2021.\\n[65] N. Hewage and D. Meedeniya, ‘‘Machine learning operations: A survey on\\nMLOps tool support,’’ 2022, arXiv:2202.10169.\\n[66] B. Karlaš, M. Interlandi, C. Renggli, W. Wu, C. Zhang, D. M. I. Babu,\\nJ. Edwards, C. Lauren, A. Xu, and M. Weimer, ‘‘Building continuous\\nintegration services for machine learning,’’ in Proc. 26th ACM SIGKDD\\nInt. Conf. Knowl. Discovery Data Mining, Aug. 2020, pp. 2407–2415, doi:\\n10.1145/3394486.3403290.\\n31878 VOLUME 11, 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='95d6b3a5-e41c-44e5-adc3-fb72cdc037c5', embedding=None, metadata={'page_label': '31879', 'file_name': 'Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_path': '/home/ago/Learning/LLM/llamaindexRAG/Data/Machine_Learning_Operations_MLOps_Overview_Definition_and_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 2660622, 'creation_date': '2024-07-25', 'last_modified_date': '2024-07-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Kreuzberger et al.: Machine Learning Operations (MLOps): Overview, Definition, and Architecture\\nDOMINIK KREUZBERGER received the B.Sc.\\ndegree in business information systems as part of\\na dual study program from Baden-Wuerttemberg\\nCooperative State University (DHBW), Stuttgart,\\nand the M.Sc. degree in information systems\\nengineering and management with a focus on\\ndigital services and machine learning operations\\n(MLOps) from the Karlsruhe Institute of Technol-\\nogy (KIT). He is currently an IT architect, spe-\\ncializing in hybrid cloud computing and artificial\\nintelligence solutions. He is also with IBM with a focus on client success,\\nwhere he designs and builds enterprise-grade data and machine-learning\\nproducts based on IBM technology. Before joining IBM, he worked for\\nnearly a decade with the multinational sports company adidas. During this\\ntime, he held various positions in the area of e-commerce and data and\\nanalytics.\\nNIKLAS KÜHL received the Ph.D. degree (summa\\ncum laude) in information systems with a focus\\non applied machine learning. In his studies, he is\\nworking on conceptualizing, designing, and imple-\\nmenting artificial intelligence (AI) products with\\na focus on inter-organizational learning as well as\\nfair and effective collaboration within human–AI\\nteams. He is currently a Full Professor of infor-\\nmation systems and human-centric AI with the\\nUniversity of Bayreuth. He is also a Group Lead\\nwith Fraunhofer FIT for business analytics as well as a Senior Expert in\\nartificial intelligence with IBM. In the past, he was a Managing Consultant\\nfor Data Science with IBM, which complemented his theoretical knowledge\\nwith practical insights from the field. He has been working on machine\\nlearning (ML) and AI in different domains, since 2014. He is internationally\\ncollaborating with multiple institutions such as the University of Texas and\\nthe MIT–IBM Watson AI Laboratory.\\nSEBASTIAN HIRSCHL is currently a Senior\\nEngineer/Architect with IBM and leads the\\nML engineering and platform activities for the\\nmachine learning practice in Germany. He com-\\nbines his computer science background in machine\\nlearning and artificial intelligence. He developed\\nthe discipline of machine learning (ML) engi-\\nneering within IBM over the last five years,\\nincluding best practices, methods, roles, and tools.\\nHe designs and leads the implementation of\\nenterprise-grade data and ML products for clients in Germany and Europe.\\nTogether with a team, he publishes the IBM Data Science Best Practices as\\nwell as shapes the IBM Data and AI reference architecture. In his role as an\\nML engineering expert, he drives the evolution of the paradigm of MLOps\\ninternally and externally.\\nVOLUME 11, 2023 31879', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex\n\u001b[0;32m----> 3\u001b[0m vector_index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m vector_index\u001b[38;5;241m.\u001b[39mas_query_engine()\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/llama_index/core/indices/base.py:145\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[1;32m    138\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    139\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     transformations,\n\u001b[1;32m    141\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    143\u001b[0m )\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:75\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     69\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m embed_model_from_settings_or_context(Settings, service_context)\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/llama_index/core/indices/base.py:94\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 94\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:308\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    301\u001b[0m     node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    302\u001b[0m ):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot build index from nodes with no content. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure all nodes have content.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:280\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     run_async_tasks(tasks)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:233\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[0;32m--> 233\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(nodes_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:141\u001b[0m, in \u001b[0;36mVectorStoreIndex._get_node_with_embedding\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_node_with_embedding\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    132\u001b[0m     nodes: Sequence[BaseNode],\n\u001b[1;32m    133\u001b[0m     show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    134\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    Allows us to store these nodes in a vector store.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    Embeddings are called in batches.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     id_to_embed_map \u001b[38;5;241m=\u001b[39m \u001b[43membed_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/llama_index/core/indices/utils.py:138\u001b[0m, in \u001b[0;36membed_nodes\u001b[0;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         id_to_embed_map[node\u001b[38;5;241m.\u001b[39mnode_id] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39membedding\n\u001b[0;32m--> 138\u001b[0m new_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embedding_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts_to_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[1;32m    143\u001b[0m     id_to_embed_map[new_id] \u001b[38;5;241m=\u001b[39m text_embedding\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:332\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding_batch\u001b[0;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    324\u001b[0m     EmbeddingStartEvent(\n\u001b[1;32m    325\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39mmodel_dict,\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    329\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING,\n\u001b[1;32m    330\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()},\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 332\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     result_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    334\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    335\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    336\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: cur_batch,\n\u001b[1;32m    337\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: embeddings,\n\u001b[1;32m    338\u001b[0m         },\n\u001b[1;32m    339\u001b[0m     )\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:432\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get text embeddings.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03mBy default, this is a wrapper around _get_text_embedding.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03mCan be overridden for batch queries.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    431\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[0;32m--> 432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/tenacity/__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/tenacity/__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/tenacity/__init__.py:418\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/tenacity/__init__.py:185\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/tenacity/__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:180\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(client, list_of_text, engine, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_text) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch size should not be larger than 2048.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m list_of_text \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m list_of_text]\n\u001b[0;32m--> 180\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_of_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [d\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/openai/resources/embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/openai/_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1263\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1264\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/openai/_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    935\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/openai/_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1030\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/openai/_base_client.py:1079\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/openai/_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1030\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/openai/_base_client.py:1079\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: SyncAPIClient._request at line 1031 (7 times), SyncAPIClient._retry_request at line 1079 (7 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/openai/_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1030\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/openai/_base_client.py:1079\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/LLM/.venv/lib/python3.11/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1045\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1049\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1050\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1054\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "vector_index.as_query_engine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
